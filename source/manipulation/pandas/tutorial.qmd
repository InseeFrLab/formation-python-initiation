::: {.content-visible when-profile="fr"}
---
title: "Traiter des données tabulaires avec Pandas"
---

L'analyse statistique a généralement pour base des **données tabulaires**, dans lesquelles chaque ligne représente une observation et chaque colonne une variable. Pour traiter ce type de données et y appliquer facilement les méthodes d'analyse de données standards, des objets dédiés ont été conçus : les `DataFrames`. Les utilisateurs de `R` connaissent bien cette structure de données, qui est native à ce langage orienté statistique. En `Python`, langage généraliste, cet objet n'existe pas nativement. Heureusement, une librairie très complete et bien pratique, pensée comme une surcouche à `NumPy`, introduit en `Python` l'objet `DataFrame` et permet la manipulation et l'analyse de données de manière simple et intuitive : `Pandas`.

::: {.callout-note}
Pandas étant l'élément central de l'éco-système data science en Python, il offre des possibilités de traitement de la donnée quasi-infinies. En plus de ça, il existe généralement de multiples manières de réaliser une même opération en Pandas. En conséquence, ce chapitre est particulièrement long et dense en nouvelles fonctionnalités. L'objectif n'est pas de retenir toutes les méthodes présentées tout au long de ce chapitre, mais plutôt d'avoir une vision générale de ce qu'il est possible de faire afin de pouvoir mobiliser les bons outils dans les projets. En particulier, les exercices de fin de chapitre et les mini-projets de fin de formation seront l'occasion d'appliquer ces nouvelles connaissances à des problématiques concrètes. 
:::

On commence par importer la librairie `Pandas`. L'usage est courant est de lui attribuer l'alias `pd` afin de simplifier les futurs appels aux objets et fonctions du package. On importe également `NumPy` car on va comparer les objets fondamentaux des deux packages.

```{python}
import pandas as pd
import numpy as np
```





## Structures de données

Pour bien comprendre le fonctionnement de `Pandas`, il faut s'intéresser à ses objets fondamentaux. On va donc d'abord étudier les `Series`, dont la concaténation permet de construire un `DataFrame`. 

### La `Series`

Une Series est un conteneur de données unidimensionnel pouvant accueillir n'importe quel type de données (entiers, *strings*, objets Python...). Une Series est néanmoins d'un type donné : une Series ne contenant que des entiers sera de type `int`, et une Series contenant des objets de différente nature sera de type `object`. Construisons notre première Series à partir d'une liste pour vérifier ce comportement.

```{python}
l = [1, "X", 3]
s = pd.Series(l)
print(s)
```

On peut notamment accéder aux données d'une Series par position, comme pour une liste ou un array.

```{python}
print(s[1])
```

A priori, on ne voit pas beaucoup de différence entre une Series et un *array* `NumPy` à 1 dimension. Pourtant, il existe une différence de taille qui est la présence d'un index : les observations ont un label associé. Lorsqu'on crée une Series sans rien spécifier, l'index est automatiquement fixé aux entiers de 0 à n-1 (avec n le nombre d'éléments de la Series). Mais il est possible de passer un index spécifique (ex : des dates, des noms de communes, etc.).

```{python}
s = pd.Series(l, index=["a", "b", "c"])
print(s)
```

Ce qui permet d'accéder aux données par label :

```{python}
s["b"]
```

Cette différence apparaît secondaire à première vue, mais deviendra essentielle pour la construction du DataFrame. Pour le reste, les Series se comportent de manière très proche des arrays NumPy : les calculs sont vectorisés, on peut directement faire la somme de deux Series, etc. D'ailleurs, on peut très facilement convertir une Series en array via l'attribut `values`. Ce qui, naturellement, fait perdre l'index...

```{python}
s = pd.Series(l, index=["a", "b", "c"])
s.values
```

### Le `DataFrame`

Fondamentalement, un DataFrame consiste en une collection de Series, alignées par les index. Cette concaténation construit donc une table de données, dont les Series correspondent aux colonnes, et dont l'index identifie les lignes. La figure suivante ([source](https://www.geeksforgeeks.org/creating-a-pandas-dataframe/)) permet de bien comprendre cette structure de données.

![](img/structure_df.png){fig-align="center" width="800"}

Un DataFrame peut être construit de multiples manières. En pratique, on construit généralement un DataFrame directement à partir de fichiers de données tabulaires (ex : CSV, excel), rarement à la main. On illustrera donc seulement la méthode de construction manuelle la plus usuelle : à partir d'un dictionnaire de données.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df
```

Un DataFrame Pandas dispose d'un ensemble d'attributs utiles que nous allons découvrir tout au long de ce tutoriel. Pour l'instant, intéressons-nous aux plus basiques : l'index et le nom des colonnes. Par défaut, l'index est initialisé comme pour les Series à la liste des positions des observations. On aurait pu spécifier un index alternatif lors de la construction du DataFrame en spécifiant l'argument `index` de la fonction `pd.DataFrame`.

```{python}
df.index
```

```{python}
df.columns
```

Souvent, plutôt que de spécifier un index à la main lors de la construction du DataFrame, on va vouloir utiliser une certaine colonne du DataFrame comme index. On utilise pour cela la méthode `set_index` associée aux DataFrames.

```{python}
df = df.set_index("date")
df
```

L'attribut index a naturellement changé :

```{python}
df.index
```





## Sélectionner des données

Lors de la manipulation des données tabulaires, il est fréquent de vouloir extraire des colonnes spécifiques d'un `DataFrame`. Cette extraction est simple avec `Pandas` grâce à l'utilisation des crochets.

### Sélectionner des colonnes

#### Sélectionner une seule colonne

Pour extraire une seule colonne, on peut utiliser la syntaxe suivante :

```{python}
selected_column = df["var1"]
selected_column
```

L'objet `selected_column` renvoie ici la colonne nommée `var1` du `DataFrame` `df`. Mais de quel type est cet objet ? Pour répondre à cette question, on utilise la fonction `type()` :

```{python}
type(selected_column)
```

Comme on peut le voir, le résultat est une `Series`, qui est un objet unidimensionnel dans `Pandas`.

Un autre attribut utile à connaître est `shape`. Il permet de connaître la dimension de l'objet. Pour une `Series`, `shape` retournera un tuple dont le premier élément indique le nombre de lignes.
```{python}
selected_column.shape
```

#### Sélectionner plusieurs colonnes

Pour extraire plusieurs colonnes, il suffit de passer une liste des noms des colonnes souhaitées :

```{python}
selected_columns = df[["var1", "var2", "experiment"]]
selected_columns
```

Cet extrait montre les colonnes `var1`, `var2` et `experiment` du `DataFrame` `df`. Vérifions maintenant son type :

```{python}
type(selected_columns)
```

Le résultat est un `DataFrame`, car il s'agit d'un objet bidimensionnel. On peut aussi vérifier sa forme avec l'attribut `shape`. Dans ce cas, le tuple renvoyé par `shape` contiendra deux éléments : le nombre de lignes et le nombre de colonnes.

```{python}
selected_columns.shape
```

### Sélectionner des lignes

#### Utilisation de `loc` et `iloc`

Lorsqu'on veut sélectionner des lignes spécifiques dans un DataFrame, on peut se servir des deux principales méthodes : `loc` et `iloc`.


- `iloc` permet de sélectionner des lignes et des colonnes par leur position, c'est-à-dire par des indices numériques.
  
Exemple, sélection des 3 premières lignes : 
```{python}
df.iloc[0:3, :]
```


- `loc` quant à lui, fonctionne avec des labels. Si les index du DataFrame sont des numéros, ils ressemblent aux positions, mais ce n'est pas forcément le cas. Il est crucial de noter que, contrairement à `iloc`, avec `loc`, l'index de fin est inclus dans la sélection.
  
```{python}
df.loc["2022-01-01":"2022-01-03", :]
```

#### Filtrage des données selon des conditions

En pratique, plutôt que de sélectionner des lignes basées sur des positions ou des labels, on souhaite souvent filtrer un DataFrame selon certaines conditions. Dans ce cas, on se sert principalement de filtres booléens.


- **Inégalités** : On peut vouloir garder seulement les lignes qui respectent une certaine condition. 

Exemple, filtrer les lignes où la valeur de la colonne `var2` est supérieure à 0 : 
```{python}
df[df['var2'] >= 0]
```


- **Appartenance avec `isin`** : Si on veut filtrer les données basées sur une liste de valeurs possibles, la méthode `isin` est très utile.

Exemple, pour garder uniquement les lignes où la colonne `experiment` a des valeurs 'test' ou 'validation' :
```{python}
df[df['experiment'].isin(['train', 'validation'])]
```

Ces méthodes peuvent être combinées pour créer des conditions plus complexes. Il est aussi possible d'utiliser les opérateurs logiques (`&` pour "et", `|` pour "ou") pour combiner plusieurs conditions. Attention, il faut bien prendre soin d'encadrer chaque condition par des parenthèses lors de la combinaison.

Exemple, sélectionner les lignes où `var2` est supérieur à 0 et `experiment` est égal à 'test' ou 'validation':
```{python}
df[(df['var2'] >= 0) & (df['experiment'].isin(['train', 'validation']))]
```





## Explorer des données tabulaires

En statistique publique, le point de départ n'est généralement pas la génération manuelle de données, mais plutôt des fichiers tabulaires préexistants. Ces fichiers, qu'ils soient issus d'enquêtes, de bases administratives ou d'autres sources, constituent la matière première pour toute analyse ultérieure. Pandas offre des outils puissants pour importer ces fichiers tabulaires et les explorer en vue de manipulations plus poussées.

### Importer et exporter des données

#### Importer un fichier CSV

Comme nous l'avons vu dans un précédent TP, le format CSV est l'un des formats les plus courants pour stocker des données tabulaires. Nous avons précédemment utilisé la librairie `csv` pour les manipuler comme des fichiers texte, mais ce n'était pas très pratique. Pour rappel, la syntaxe pour lire un fichier CSV et afficher les premières lignes était la suivante : 

```{python}
import csv

rows = []

with open("data/departement2021.csv") as file_in:
    csv_reader = csv.reader(file_in)
    for row in csv_reader:
        rows.append(row)

rows[:5]
```

Avec Pandas, il suffit d'utiliser la fonction `read_csv()` pour importer le fichier comme un DataFrame, puis la fonction `head()`.

```{python}
df_departements = pd.read_csv('data/departement2021.csv')
df_departements.head()
```

Il est également possible d'importer un fichier CSV directement à partir d'une URL. C'est particulièrement pratique lorsque les données sont régulièrement mises à jour sur un site web et que l'on souhaite accéder à la version la plus récente sans avoir à télécharger manuellement le fichier à chaque fois. Prenons l'exemple d'un fichier CSV disponible sur le site de l'INSEE : le fichier des prénoms, issu des données de l'état civil. On note au passage une autre fonctionnalité bien pratique : le fichier CSV est compressé (format `zip`), mais Pandas est capable de le reconnaître et de le décompresser avant de l'importer.

```{python}
# Importer un fichier CSV depuis une URL
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_prenoms_url = pd.read_csv(url, sep=";")
df_prenoms_url.head()
```

Lorsqu'on travaille avec des fichiers CSV, il y a de nombreux arguments optionnels disponibles dans la fonction `read_csv()` qui permettent d'ajuster le processus d'importation en fonction des spécificités du fichier. Ces arguments peuvent notamment permettre de définir un délimiteur spécifique (comme ci-dessus pour le fichier des prénoms), de sauter certaines lignes en début de fichier, ou encore de définir les types de données pour chaque colonne, et bien d'autres. Tous ces paramètres et leur utilisation sont détaillés dans la [documentation officielle](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

#### Exporter au format CSV

Une fois que les données ont été traitées et modifiées au sein de Pandas, il est courant de vouloir exporter le résultat sous forme de fichier CSV pour le partager, l'archiver ou l'utiliser dans d'autres outils. Pandas offre une méthode simple pour cette opération : `to_csv()`. Supposons par exemple que l'on souhaite exporter les données du DataFrame `df_departements` spécifiques aux cinq départements d'outre-mer.

```{python}
df_departements_dom = df_departements[df_departements["DEP"].isin(["971", "972", "973", "974", "975"])]
df_departements_dom.to_csv('output/departements2021_dom.csv')
```

Un des arguments clés de la méthode `to_csv()` est `index`. Par défaut, `index=True`, ce qui signifie que l'index du DataFrame sera également écrit dans le fichier CSV. On peut le vérifier en imprimant les premières lignes de notre fichier CSV : Pandas a ajouté une colonne non-nommée, qui contient l'index des lignes retenues.

```{python}
with open("output/departements2021_dom.csv") as file_in:
    for i in range(5):
        row = next(file_in).strip()
        print(row)
```

Dans certains cas, notamment lorsque l'index n'apporte pas d'information utile ou est simplement généré automatiquement par Pandas, on pourrait vouloir l'exclure du fichier exporté. Pour ce faire, on peut définir `index=False`.

```{python}
df_departements_dom.to_csv('output/departements2021_dom_noindex.csv', index=False)
```

#### Importer un fichier Parquet

Le format Parquet est un autre format pour le stockage de données tabulaires, de plus en plus fréquemment utilisé. Sans entrer dans les détails techniques, le format Parquet présente différentes caractéristiques qui en font un choix privilégié pour le stockage et le traitement de gros volumes de données. En raison de ces avantages, ce format est de plus en plus utilisé pour la mise à disposition de données à l'Insee. Il est donc essentiel de savoir importer et requêter des fichiers Parquet avec Pandas.

Importer un fichier Parquet dans un DataFrame Pandas se fait tout aussi facilement que pour un fichier CSV. La fonction se nomme `read_parquet()`.

```{python}
df_departements = pd.read_parquet('data/departement2021.parquet')
df_departements.head()
```

#### Exporter au format Parquet

Là encore, tout se passe comme dans le monde des CSV : on utilise la méthode `to_parquet()` pour exporter un DataFrame dans un fichier Parquet. De même, on peut choisir d'exporter ou non l'index, à l'aide du paramètre `index` (qui vaut `True` par défaut).

```{python}
df_departements_dom = df_departements[df_departements["DEP"].isin(["971", "972", "973", "974", "975"])]
df_departements_dom.to_parquet('output/departements2021_dom.parquet', index=False)
```

Une des grandes forces du format Parquet, en comparaison des formats texte comme le CSV, est sa capacité à stocker des méta-données, i.e. des données permettant de mieux comprendre les données contenues dans le fichier. En particulier, un fichier Parquet inclut dans ses méta-données le schéma des données (noms des variables, types des variables, etc.), ce qui en fait un format très adapté à la diffusion de données. Vérifions ce comportement en reprenant le DataFrame que nous avons défini précédemment.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df = df.assign(
    experiment=pd.Categorical(df["experiment"]),
    date=pd.to_datetime(df["date"])
)
```

On utilise cette fois deux types de données spécifiques, pour les données catégorielles (`category`) et pour les données temporelles (`datetime`). On verra plus loin dans le tutoriel comment utiliser ces types. Pour l'instant, notons simplement que Pandas stocke ces types dans le schéma des données.

```{python}
df.info()
```

Vérifions à présent que l'export et le ré-import de ces données en Parquet préserve le schéma.

```{python}
df.to_parquet("output/df_test_schema.parquet", index=False)
df_test_schema_parquet = pd.read_parquet('output/df_test_schema.parquet')

df_test_schema_parquet.info()
```

A l'inverse, un fichier CSV ne contenant par définition que du texte, ne permet pas de préserver ces données. Les variables dont nous avons spécifié le type sont importées comme des strings (type `object` en Pandas).

```{python}
df.to_csv("output/df_test_schema.csv", index=False)
df_test_schema_csv = pd.read_csv('output/df_test_schema.csv')

df_test_schema_csv.info()
```

### Visualiser un échantillon des données

Lorsqu'on travaille avec des jeux de données volumineux, il est souvent utile de visualiser rapidement un échantillon des données pour avoir une idée de leur structure, de leur format ou encore pour détecter d'éventuels problèmes. Pandas offre plusieurs méthodes pour cela.

La méthode `head()` permet d'afficher les premières lignes du DataFrame. Par défaut, elle retourne les 5 premières lignes, mais on peut spécifier un autre nombre en argument si nécessaire.

```{python}
df_departements.head()
```

```{python}
df_departements.head(10)
```

À l'inverse, la méthode `tail()` donne un aperçu des dernières lignes du DataFrame.

```{python}
df_departements.tail()
```

L'affichage des premières ou dernières lignes peut parfois ne pas être représentatif de l'ensemble du jeu de données, lorsque les données sont triées par exemple. Afin de minimiser le risque d'obtenir un aperçu biaisé des données, on peut utiliser la méthode `sample()`, qui sélectionne un un échantillon aléatoire de lignes. Par défaut, elle retourne une seule ligne, mais on peut demander un nombre spécifique de lignes en utilisant l'argument `n`.

```{python}
df_departements.sample(n=5)
```

### Obtenir une vue d'ensemble des données

L'une des premières étapes lors de l'exploration de nouvelles données est de comprendre la structure générale du jeu de données. La méthode `info()` de Pandas offre une vue d'ensemble rapide des données, notamment en termes de types de données, de présence de valeurs manquantes et de mémoire utilisée.

```{python}
df.info()
```

Plusieurs éléments d'information clés peuvent être extraits de ce résultat :

- **index** : le DataFrame a un `RangeIndex`, ce qui signifie que l'index est constitué d'une suite numérique simple. Ici, l'index va de 0 à 5, soit 6 entrées au total.

- **schéma** : la liste des colonnes est affichée avec des informations très utiles sur le schéma des données :

  - **Non-Null Count** : le nombre de valeurs **non-manquantes** (non `nan`) dans la colonne. Si ce nombre est inférieur au nombre total d'entrées (dans notre cas, 6), cela signifie que la colonne contient des valeurs manquantes. Attention à l'ambiguité possible sur "null" : cela signifie bien les valeurs manquantes, pas les valeurs égales à 0. Ainsi, dans notre cas, le nombre de valeurs "non-null" pour la variable `var1` est 5.

  - **Dtype** : Le type de données de la colonne, qui permet decomprendre la nature des informations stockées dans chaque colonne. Par exemple, `float64` (nombres réels), `int32` (nombres entiers), `category` (variable catégorielle), `datetime64[ns]` (information temporelle) et `object` (données textuelles ou mixtes).   

L'utilisation de `info()` est un moyen rapide et efficace d'obtenir une vue d'ensemble d'un DataFrame, d'identifier rapidement les colonnes contenant des valeurs manquantes et de comprendre la structure des données.

### Calculer des statistiques descriptives

En complément des informations renvoyées par la méthode `info()`, on peut vouloir obtenir des statistiques descriptives simples afin de visualiser rapidement les distributions des variables. La méthode `describe()` permet d'avoir une vue synthétique de la distribution des données dans chaque colonne. 

```{python}
df.describe()
```

Il est à noter que `describe()` ne renvoie des statistiques que pour les colonnes numériques par défaut. Si l'on souhaite inclure des colonnes d'autres types, il est nécessaire de le préciser via l'argument `include`. Par exemple, `df.describe(include='all')` renverra des statistiques pour toutes les colonnes, y compris des métriques comme le nombre unique, la valeur la plus fréquente et la fréquence de la valeur la plus fréquente pour les colonnes non numériques.

```{python}
df.describe(include='all')
```

Notons que, là encore, la variable `count` renvoie le nombre de valeurs **non-manquantes** dans chaque variable.





## Principales manipulations de données

### Transformer les données

Les opérations de transformation sur les données sont essentielles pour façonner, nettoyer et préparer les données en vue de leur analyse. Les transformations peuvent concerner l'ensemble du DataFrame, des colonnes spécifiques ou encore des lignes spécifiques.

#### Transformer un DataFrame

Pour transformer un DataFrame complet (ou un sous-DataFrame), il est possible d'utiliser des fonctions vectorisées, qui permettent d'appliquer rapidement une opération à l'ensemble des éléments du DataFrame. Cela inclut un certain nombre de méthodes disponibles pour les `Series`, mais aussi les fonctions mathématiques de `NumPy`, etc.

Par exemple, passer chaque valeur numérique d'un DataFrame à la puissance 2 :

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
    }
)

df ** 2
```

ou les passer en valeur absolue :

```{python}
np.abs(df)
```

Certaines méthodes, disponibles pour les `Series`, peuvent aussi être utilisées pour transformer un DataFrame complet. Par exemple, la bien utile méthode [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html), qui permet de remplacer toutes les occurences d'une valeur donnée par une autre valeur. Par exemple, supposons que la valeur 0 dans la colonne `var1` indique en fait une erreur de mesure. Il serait préférable de la remplacer par une valeur manquante.

```{python}
df.replace(0, np.nan)
```

::: {.callout-warning title="Assignation ou méthodes *in place* (en place) ?"}
Dans l'exemple précédent, l'application de la méthode [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) ne modifie pas directement le DataFrame. Pour que la modifiction soit persistente, une première possibilité est d'assigner le résultat à un objet : 

```{python}
df = df.replace(0, np.nan)
```

Une seconde possibilité est, lorsque les méthodes le proposent, d'utiliser l'argument `inplace`. Lorsque `inplace=True`, l'opération est effectuée "en place", et le DataFrame est donc modifié directement.

```{python}
df.replace(0, np.nan, inplace=True)
```

En pratique, il est préférable de limiter les opérations `inplace`. Elles ne favorisent pas la reproductibilité des analyses, dans la mesure où la ré-exécution d'une même cellule va donner à chaque fois des résultats différents.
:::

#### Transformer les colonnes

Dans certains cas, on ne va pas vouloir appliquer les transformations à l'ensemble des données, mais à des variables spécifiques. Les transformations qui sont possibles à l'échelle du DataFrame (fonctions vectorisées, méthodes comme [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html), etc.) restent naturellement possibles à l'échelle d'une colonne.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
    }
)

np.abs(df["var2"])
```

```{python}
df["var1"].replace(0, np.nan)
```

Mais il existe d'autres transformations que l'on applique généralement au niveau d'une ou de quelques colonnes. Par exemple, lorsque le schéma n'a pas été bien reconnu à l'import, il peut arriver que des variables numériques soient définies comme des string (type `object` en Pandas).

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan],
        "var2": ["1", "5", "18"],
    }
)

df.info()
```

Dans ce cas, on peut utiliser la méthode `astype` pour convertir la colonne dans le type souhaité.

```{python}
df['var2'] = df['var2'].astype(int)

df.info()
```

Une autre opération fréquente est le renommage d'une ou plusieurs colonnes. Pour cela, on peut utiliser la méthode [rename()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html), à laquelle on passe un dictionnaire qui contient autant de couples clé-valeur que de variables à renommer, et dans lequel chaque couple clé-valeur est de la forme `'ancien_nom': 'nouveau_nom'`.

```{python}
df.rename(columns={'var2': 'age'})
```

Enfin, on peut souhaiter supprimer du DataFrame des colonnes qui ne sont pas ou plus utiles à l'analyse. Pour cela, on utilise la méthode [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html), à laquelle on passe soit un string (nom d'une colonne si l'on souhaite n'en supprimer qu'une seule) ou une liste de noms de colonne à supprimer.

```{python}
df.drop(columns=['var1'])
```

#### Transformer les lignes

En statistiques, on applique généralement des tranformations faisant intervenir une ou plusieurs colonnes. Néanmoins, dans certains cas, il est nécessaire d'appliquer des transformations au niveau des lignes. Pour cela, on peut utiliser la méthode [apply()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) de Pandas, appliquée à l'axe des lignes (`axis=1`). Illustrons son fonctionnement avec un cas simple. Pour cela, on génère d'abord des données.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
    }
)

df.head()
```

On applique maintenant la fonction `apply()` au DataFrame afin de calculer une nouvelle variable qui est la somme des deux existantes.

```{python}
df['sum_row'] = df.apply(lambda row: row['var1'] + row['var2'], axis=1)

df.head()
```

::: {.callout-tip title="Les fonctions lambda"}
Une fonction `lambda` est une petite fonction anonyme. Elle peut prendre n'importe quel nombre d'arguments, mais ne peut avoir qu'une seule expression. Dans l'exemple ci-dessus, la fonction `lambda` prend une ligne en argument et renvoie la somme des colonnes `var1` et `var2` pour cette ligne. 

Les fonctions `lambda` permettent de définir simplement des fonctions "à la volée", sans devoir leur donner un nom. Dans notre exemple, cela aurait été parfaitement équivalent au code suivant : 

```{python}
def sum_row(row):
    return row['var1'] + row['var2']

df['sum_row'] = df.apply(sum_row, axis=1)
```
:::

Bien que `apply()` offre une grande flexibilité, elle n'est pas la méthode la plus efficiente, notamment pour de grands jeux de données. Les opérations vectorisées sont toujours préférables car elles traitent les données en bloc plutôt que ligne par ligne. Dans notre cas, il aurait été bien entendu préférable de créer notre variable en utilisant des opérations sur les colonnes.

```{python}
df['sum_row_vect'] = df['var1'] + df['var2']

df.head()
```

Néanmoins, on peut se retrouver dans certains (rares) cas où une opération ne peut pas être facilement vectorisée ou où la logique est complexe. Supposons par exemple que l'on souhaite combiner les valeurs de plusieurs colonnes en fonction de certaines conditions.

```{python}
def combine_columns(row):
    if row['var1'] > 6:
        return str(row['var2'])
    else:
        return str(row['var2']) + "_" + row['date']

df['combined_column'] = df.apply(combine_columns, axis=1)

df
```

### Trier les valeurs

Le tri des données est particulièrement utile pour l'exploration et la visualisation de données. Avec Pandas, on utilise la méthode [sort_values()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) pour trier les valeurs d'un DataFrame selon une ou plusieurs colonnes.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
    }
)

df
```

Pour trier les valeurs selon une seule colonne, il suffit de passer le nom de la colonne en paramètre.

```{python}
df.sort_values(by='var1')
```

Par défaut, le tri est effectué dans l'ordre croissant. Pour trier les valeurs dans un ordre décroissant, il suffit de paramétrer `ascending=False`.

```{python}
df.sort_values(by='var1', ascending=False)
```

Si on souhaite trier le DataFrame sur plusieurs colonnes, on peut fournir une liste de noms de colonnes. On peut également choisir de trier de manière croissante pour certaines colonnes et décroissante pour d'autres.



### Agréger des données

L'agrégation des données est un processus dans lequel les données vont être ventilées en groupes selon certains critères, puis agrégées selon une fonction d'agrégation appliquée indépendamment à chaque groupe. Cette opération est courante lors de l'analyse exploratoire ou lors du prétraitement des données pour la visualisation ou la modélisation statistique.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df.head()
```

#### L'opération `groupBy`

La méthode `groupBy` de Pandas permet de diviser le DataFrame en sous-ensembles selon les valeurs d'une ou plusieurs colonnes, puis d'appliquer une fonction d'agrégation à chaque sous-ensemble. Elle renvoie un objet de type `DataFrameGroupBy` qui ne présente pas de grand intérêt en soi, mais constitue l'étape intermédiaire indispensable pour pouvoir ensuite appliquer une ou plusieurs fonction(s) d'agrégation aux différents groupes.

```{python}
df.groupby('experiment')
```

#### Fonctions d'agrégation

Une fois les données groupées, on peut appliquer des fonctions d'agrégation pour obtenir un résumé statistique. Pandas intègre un certain nombre de ces fonctions, dont la liste complète est détaillée dans la [documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-aggregation-methods). Voici quelques exemples d'utilisation de ces méthodes.

Par exemple, compter le nombre d'occurrences dans chaque groupe.

```{python}
df.groupby('experiment').size()
```

Calculer la somme d'une variable par groupe.

```{python}
df.groupby('experiment')['var1'].sum()
```

Ou encore compter le nombre de valeurs unique d'une variable par groupe. Les possibilités sont nombreuses.

```{python}
# Pour le nombre de valeurs uniques de 'var2' dans chaque groupe
df.groupby('experiment')['var2'].nunique()
```

Lorsqu'on souhaite appliquer plusieurs fonctions d'agrégation à la fois ou des fonctions personnalisées, on utilise la méthode `agg`. Cette méthode accepte une liste de fonctions ou un dictionnaire qui associe les noms des colonnes aux fonctions à appliquer. Cela permet d'appliquer plus finement les fonctions d'agrégation.

```{python}
df.groupby('experiment').agg({'var1': 'mean', 'var2': 'count'})
```

::: {.callout-note title="Le chaînage de méthodes"}
Les exemples précédents illustrent un concept important en Pandas : le chaînage de méthodes. Ce terme désigne la possibilité d'enchaîner les transformations appliquées à un DataFrame en lui appliquant à la chaîne des méthodes. A chaque méthode appliquée, un DataFrame intermédiaire est créé (mais non assigné à une variable), qui devient l'input de la méthode suivante.

Le chaînage de méthodes permet de combiner plusieurs opérations en une seule expression de code. Cela peut améliorer l'efficacité en évitant les assignations intermédiaires et en rendant le code plus fluide et plus facile à lire.
Cela favorise également un style de programmation fonctionnel où les données passent à travers une chaîne de transformations de manière fluide.
:::

#### Effets sur l'index

Il est intéressant de noter les effets du processus d'agrégation sur l'index du DataFrame. Le dernier exemple ci-dessus l'illustre bien : les groupes, i.e. les modalités de la variable utilisée pour effectuer l'agrégation, deviennent les valeurs de l'index.

On peut vouloir réutiliser cette information dans des analyses ultérieures, et donc la vouloir comme une colonne. Il suffit pour cela de réinitialiser l'index avec la méthode `reset_index()`.

```{python}
df_agg = df.groupby('experiment').agg({'var1': 'mean', 'var2': 'count'})
df_agg.reset_index()
```

### Traiter les valeurs manquantes

Les valeurs manquantes sont une réalité courante dans le traitement des données réelles et peuvent survenir pour diverses raisons, telles que des non-réponses à un questionnaire, des erreurs de saisie, des pertes de données lors de la transmission ou simplement parce que l'information n'est pas applicable. Pandas offre plusieurs outils pour gérer les valeurs manquantes.

#### Représentation des valeurs manquantes

Dans Pandas, les valeurs manquantes sont généralement représentées par `np.nan`, qui est un marqueur spécial fourni par la bibliothèque `NumPy`. S'il est préférable d'utiliser cet objet pour dénoter les valeurs manquantes, notons que l'objet `None` de `Python` est également compris comme une valeur manquante par `Pandas`.

Vérifions cette propriété. Pour identifier où se trouvent les valeurs manquantes, on utilise la fonction `isna()` qui retourne un DataFrame booléen indiquant `True` là où les valeurs sont `NaN`.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
        "sample": "sample1"
    }
)

df.isna()
```

#### Calculs sur des colonnes contenant des valeurs manquantes

Lors de calculs statistiques, les valeurs manquantes sont généralement ignorées. Par exemple, la méthode `.mean()` calcule la moyenne des valeurs non manquantes.

```{python}
df['var1'].mean()
```

En revanche, les calculs faisant intervenir plusieurs colonnes n'ignorent pas toujours les valeurs manquantes et peuvent souvent donner des résultats en `NaN`.

```{python}
df['var3'] = df['var1'] + df['var2']

df
```

#### Suppression des valeurs manquantes

La méthode `dropna()` permet de supprimer les lignes (`axis=0`) ou les colonnes (`axis=1`) contenant des valeurs manquantes. Par défaut, toute ligne contenant au moins une valeur manquante est supprimée.

```{python}
df.dropna()
```

En modifiant le paramètre `axis`, on peut demander à ce que toute colonne contenant au moins une valeur manquante soit supprimée.

```{python}
df.dropna(axis=1)
```

Enfin, le paramètre `how` définit la modalité de supression. Par défaut, une ligne ou colonne est supprimée lorsqu'au moins une valeur est manquante (`how=any`), mais il est possible de ne supprimer la ligne/colonne que lorsque toutes les valeurs sont manquantes (`how=all`).

#### Remplacement des valeurs manquantes

Pour gérer les valeurs manquantes dans un DataFrame, une approche commune est l'imputation, qui consiste à remplacer les valeurs manquantes par d'autres valeurs. La méthode `fillna()` permet d'effectuer cette opération de différentes manières. Une première possibilité est le remplacement par une valeur constante.

```{python}
df['var1'].fillna(value=0)
```

::: {.callout-warning title="Changement de représentation des valeurs manquantes"}
On peut parfois être tentant de changer la manifestation d'une valeur manquante pour des raisons de visibilité, par exemple en la remplaçant par une chaîne de caractères :

```{python}
df['var1'].fillna(value="MISSING")
```

En pratique, cette façon de faire n'est pas recommandée. Il est en effet préférable de conserver la convention standard de `Pandas` (l'utilisation des `np.nan`), d'abord pour des questions de standardisation des pratiques qui facilitent la lecture et la maintenance du code, mais également parce que la convention standard est optimisée pour la performance et les calculs à partir de données contenant des valeurs manquantes.
:::

Une autre méthode d'imputation fréquente est d'utiliser une valeur statistique, comme la moyenne ou la médiane de la variable.

```{python}
df['var1'].fillna(value=df['var1'].mean())
```

::: {.callout-warning title="Biais d'imputation"}
Remplacer les valeurs manquantes par une valeur constante, telle que zéro, la moyenne ou la médiane, peut être problématique. Si les données ne sont pas manquantes au hasard (*Missing Not At Random* - *MNAR*), cela peut introduire un biais dans l'analyse. Les variables *MNAR* sont des variables dont la probabilité d'être manquantes est liée à leur propre valeur ou à d'autres variables dans les données. Dans de tels cas, une imputation plus sophistiquée peut être nécessaire pour minimiser les distorsions. Nous en verrons un exemple en exercice de fin de tutoriel.
:::

### Traiter les données de types spécifiques

#### Données textuelles

Les données textuelles nécessitent souvent un nettoyage et une préparation avant l'analyse. Pandas fournit via la librairie de méthodes `str` un ensemble d'opérations vectorisées qui rendent la préparation des données textuelles à la fois simple et très efficace. Là encore, les possibilités sont multiples et détaillées dans la [documentation](https://pandas.pydata.org/docs/user_guide/text.html). Nous présentons ici les méthodes les plus fréquemment utilisées dans l'analyse de données.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "test", "train", "validation"],
        "sample": ["  sample1", "sample1", "sample2", "   sample2   ", "sample2  ", "sample1"]
    }
)

df
```

Une première opération fréquente consiste à extraire certains caractères d'une chaîne. On utilise pour cela la fonction (à la syntaxe un peu particulière) `str[n:]` Par exemple, si l'on veut extraire le dernier caractère de la variable `sample` afin de ne retenir que le chiffre de l'échantillon.

```{python}
df["sample_n"] = df["sample"].str[-1:]

df
```

Le principe était le bon, mais la présence d'espaces superflus dans nos données textuelles (qui ne se voyaient pas à la visualisation du DataFrame !) a rendu l'opération plus difficile que prévue. C'est l'occasion d'introduire la famille de méthode `strip` (`.str.strip()`, `.str.lstrip()` et `.str.rstrip()`) qui respectivement retirent les espaces superflus des deux côtés ou d'un seul.

```{python}
df["sample"] = df["sample"].str.strip()
df["sample_n"] = df["sample"].str[-1:]

df
```

On peut également vouloir filtrer un DataFrame en fonction de la présence ou non d'une certaine chaîne (ou sous-chaîne) de caractères. On utilise pour cela la méthode `.str.contains()`.

```{python}
df[df['experiment'].str.contains('test')]
```

Enfin, on peut vouloir remplacer une chaîne (ou sous-chaîne) de caractères par une autre, ce que permet la méthode `str.replace()`.

```{python}
df['experiment'] = df['experiment'].str.replace('validation', 'val')

df
```

#### Données catégorielles

Les données catégorielles sont des variables qui contiennent un nombre restreint de modalités. A l'instar de `R` avec la notion de `factor`, Pandas a un type de données spécial, `category`, qui est utile pour représenter des données catégorielles de manière plus efficace et plus informative. Les données catégorielles sont en effet optimisées pour certains types de données et peuvent accélérer les opérations comme le groupement et le tri. Elles sont également utiles pour la visualisation, car elles permettent d'assurer que les catégories sont affichées dans un ordre cohérent et logique.

Pour convertir une variable au format `category`, on utilise la méthode `astype()`.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
    }
)
print(df.dtypes)
```

```{python}
df['experiment'] = df['experiment'].astype('category')

print(df.dtypes)
```

Cette conversion nous donne accès à quelques méthodes bien pratiques, spécifiques au traitement des variables catégorielles. Il peut par exemple être utile de renommer les catégories pour des raisons de clarté ou de standardisation.

```{python}
df['experiment'] = df['experiment'].cat.rename_categories({'test': 'Test', 'train': 'Train', 'validation': 'Validation'})
df
```

Parfois, l'ordre des catégories est significatif, et on peut vouloir le modifier. En particulier dans le cadre de la visualisation, car les modalités seront par défaut affichées dans l'ordre spécifié.

```{python}
df_cat = df['experiment'].cat.reorder_categories(['Test', 'Train', 'Validation'], ordered=True)
df.groupby("experiment").mean().plot(kind='bar')
```

#### Données temporelles

Les données temporelles sont souvent présentes dans les données tabulaires afin d'identifier temporellement les observations recueillies. Pandas offre des fonctionnalités pour manipuler ces types de données, notamment grâce au type `datetime64` qui permet une manipulation précise des dates et des heures.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2023-01-01", "2023-01-02"],
        "sample": ["sample1", "sample1", "sample2", "sample2"]
    }
)

df.dtypes
```

Pour manipuler les données temporelles, il est nécessaire de convertir les chaînes de caractères en objets `datetime`. Pandas le fait via la fonction `to_datetime()`.

```{python}
df['date'] = pd.to_datetime(df['date'])

df.dtypes
```

Une fois converties, les dates peuvent être formatées, comparées et utilisées dans des calculs. En particulier, Pandas comprend à présent l'"ordre" des dates présentes dans les données, et permet donc le filtrage sur des périodes données.

```python
df[(df['date'] >= "2022-01-01") & (df['date'] < "2022-01-03")]
```

On peut également vouloir réaliser des filtrages moins précis, faisant intervenir l'année ou le mois. Pandas permet d'extraire facilement des composants spécifiques de la date, comme l'année, le mois, le jour, l'heure, etc.

```{python}
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day

df[df['year'] == 2023]
```

Enfin, les calculs faisant intervenir des dates deviennent possible. On peut ajouter ou soustraire des périodes temporelles à des dates, et les comparer entre elles. Les fonctions utilisées sont issues de `Pandas`, mais sont très semblables dans leur fonctionnement à celles du module [time](https://docs.python.org/fr/3/library/time.html) de Python.

On peut par exemple ajouter des intervalles de temps, ou bien calculer des écarts à une date de référence.

```{python}
df['date_plus_one'] = df['date'] + pd.Timedelta(days=1)
df['date_diff'] = df['date'] - pd.to_datetime('2022-01-01')

df
```

### Joindre des tables

Dans le cadre d'une analyse de données, il est courant de vouloir combiner différentes sources de données. Cette combinaison peut se faire verticalement (un DataFrame par dessus l'autre), par exemple lorsque l'on souhaite combiner deux millésimes d'une même enquête afin de les analyser conjointement. La combinaison peut également se faire horizontalement (côte à côte) selon une ou plusieurs clé(s) de jointure, souvent dans le but d'enrichir une source de données à partir d'une autre source portant sur les mêmes unités statistiques.

#### Concaténer des tables

La concaténation verticale de tables se fait à l'aide de la fonction `concat()` de Pandas.

```{python}
df1 = pd.DataFrame(
    data = {
        "var1": [1, 5],
        "var2": [3, 7],
        "date": ["2022-01-01", "2022-01-02"],
        "sample": ["sample1", "sample1"]
    }
)

df2 = pd.DataFrame(
    data = {
        "var1": [9, 13],
        "date": ["2023-01-01", "2023-01-02"],
        "var2": [11, 15],
        "sample": ["sample2", "sample2"]
    }
)

df_concat = pd.concat([df1, df2])

df_concat
```

Notons que l'ordre des variables dans les deux DataFrames n'est pas important. Pandas ne juxtapose pas "bêtement" les deux DataFrames, il fait une correspondance des schémas pour faire correspondre les variables par nom. Si deux variables ont le même nom mais pas le même type - par exemple dans le cas où une variable numérique aurait été interprétée comme des strings - Pandas va résoudre le problème en prenant le dénominateur commun, c'est à dire en général convertir en strings (type `object`).

Par contre, la concaténation précédente laisse apparaître un problème de répétition au niveau de l'index. C'est logique : on n'a pas spécifié d'index pour nos deux DataFrames initiaux, qui ont donc le même index de position ([0, 1]). Dans ce cas (où l'index n'est pas important), on peut passer le paramètre `ignore_index=True` pour reconstruire de zéro l'index final.

```{python}
df_concat = pd.concat([df1, df2], ignore_index=True)

df_concat
```

::: {.callout-warning title="Construction itérative d'un DataFrame"}
On pourrait avoir l'idée d'utiliser `pd.concat()` pour construire un DataFrame de manière itérative, en ajoutant à chaque itération d'une boucle une nouvelle ligne au DataFrame existant. Ce n'est néanmoins pas une bonne idée : comme nous l'avons vu, un DataFrame est représenté dans la mémoire commme une juxtaposition de Series. Ainsi, ajouter une colonne à un DataFrame est peu coûteux, mais ajouter une ligne implique de modifier chaque élément constituant du DataFrame. Pour construire un DataFrame, il est donc plutôt conseillé de stocker les lignes dans une liste de listes (une par colonne) ou un dictionnaire, puis d'appeler `pd.DataFrame()` pour construire le DataFrame, comme nous l'avons fait au début de ce tutoriel.
:::

#### Fusionner des tables

La fusion de tables est une opération qui permet d'associer des lignes de deux DataFrames différents en se basant sur une ou plusieurs clés communes, similaire aux jointures dans les bases de données SQL. Différents types de jointure sont possible selon les données que l'on souhaite conserver, dont les principaux sont représentés sur le graphique suivant.

![](img/joins.png)

Source : [lien](https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d)

En Pandas, les jointures se font avec la fonction [merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html). Pour réaliser une jointure, on doit spécifier (au minimum) deux informations :

- le type de jointure : par défaut, Pandas effectue une jointure de type `inner`. Le paramètre `how` permet de spécifier d'autres types de jointure ;

- la clé de jointure. Par défaut, Pandas essaie de joindre les deux DataFrames à partir de leurs index. En pratique, on spécifie souvent une variable présente dans le DataFrames comme clé de jointure (paramètre `on` si la variable porte le même nom dans les deux DataFrame, ou `left_on` et `right_on` sinon).

Analysons la différence entre les différents types de jointure à travers des exemples.

```{python}
df_a = pd.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K3', 'K4'],
    'A': ['A0', 'A1', 'A2', 'A3', 'A4'],
    'B': ['B0', 'B1', 'B2', 'B3', 'A4']
})

df_b = pd.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K5', 'K6'],
    'C': ['C0', 'C1', 'C2', 'C5', 'C6'],
    'D': ['D0', 'D1', 'D2', 'D5', 'D6']
})

display(df_a)
display(df_b)
```

La jointure de type `inner` conserve les observations dont la clé est présente dans les deux DataFrame. 

```{python}
df_merged_inner = pd.merge(df_a, df_b, on='key')
df_merged_inner
```

::: {.callout-warning title="Jointures inner"}
La jointure de type `inner` est la plus intuitive : elle ne crée généralement pas de valeurs manquantes et permet donc de travailler directement sur la table fusionnée. Mais attention : si beaucoup de clés ne sont pas présentes dans les deux DataFrames à la fois, une jointure `inner` peut aboutit à des pertes importantes de données, et donc à des résultats finaux biaisés. Dans ce cas, il vaut mieux choisir une jointure à gauche ou à droite, selon la source que l'on cherche à enrichir et pour laquelle il est donc le plus important de limiter les pertes de données.
:::

Une jointure de type `left` conserve toutes les observations contenues dans le DataFrame de gauche (premier DataFrame spécifié dans `pd.merge()`). Par conséquent, si des clés sont présentes dans le DataFrame de gauche mais pas dans celui de droite, le DataFrame final contient des valeurs manquantes au niveau de ces observations (pour les variables du DataFrame de droite).

```{python}
df_merged_left = pd.merge(df_a, df_b, how="left", on='key')
df_merged_left
```

La jointure de type `outer` contient toutes les observations et variables contenues dans les deux DataFrame. Ainsi, l'information retenue est maximale, mais en contrepartie les valeurs manquantes peuvent être assez nombreuses. Il sera donc nécessaire de bien traiter les valeurs manquantes avant de procéder aux analyses.

```{python}
df_merged_outer = pd.merge(df_a, df_b, how="outer", on='key')
df_merged_outer
```

## Exercices

### Questions de compréhension


- 1/ Qu'est-ce qu'un DataFrame dans le contexte de Pandas et à quel type de structure de données peut-on le comparer dans le langage Python ?

- 2/ Quelle est la différence fondamentale entre un array Numpy et une Pandas Series ?

- 3/ Quel est le lien entre Series et DataFrame dans Pandas ?

- 4/ Comment sont structurées les données dans un DataFrame Pandas ?

- 5/ Quel est le rôle de l'index dans un DataFrame Pandas et comment peut-il être utilisé lors de la manipulation des données ?

- 6/ Quelles méthodes pouvez-vous utiliser pour explorer un DataFrame inconnu et en apprendre davantage sur son contenu et sa structure ?

- 7/ Dans Pandas, quelle est la différence entre assigner le résultat d'une opération à une nouvelle variable et utiliser une méthode avec l'argument `inplace=True` ?

- 8/ Comment s'applique le principe de la vectorisation dans Pandas et pourquoi est-ce avantageux pour manipuler les données ?

- 9/ Comment Pandas représente-t-il les valeurs manquantes et quel impact cela a-t-il sur les calculs et les transformations de données ?

- 10/ Quelle est la différence entre concaténer deux DataFrames et les joindre via une jointure, et quand utiliseriez-vous l'une plutôt que l'autre ?


::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

- 1/ Un DataFrame dans Pandas est une structure de données bidimensionnelle, comparable à un tableau ou une feuille de calcul Excel. Dans le contexte Python, on peut le comparer à un dictionnaire d'arrays NumPy, où les clés sont les noms des colonnes et les valeurs sont les colonnes elles-mêmes.

- 2/ La différence principale entre un array NumPy et une Series Pandas est que la Series peut contenir des données étiquetées, c'est-à-dire qu'elle a un index qui lui est associé, permettant des accès et des manipulations par label.

- 3/ Un DataFrame est essentiellement une collection de Series. Chaque colonne d'un DataFrame est une Series, et toutes ces Series partagent le même index, qui correspond aux étiquettes des lignes du DataFrame.

- 4/ Les données dans un DataFrame Pandas sont structurées en colonnes et en lignes. Chaque colonne peut contenir un type de données différent (numérique, chaîne de caractères, booléen, etc.), et chaque ligne représente une observation.

- 5/ L'index dans un DataFrame Pandas sert à identifier de manière unique chaque ligne du DataFrame. Il permet d'accéder rapidement aux lignes, de réaliser des jointures, de trier les données et de faciliter les opérations de regroupement.

- 6/ Pour explorer un DataFrame inconnu, on peut utiliser df.head() pour voir les premières lignes, df.tail() pour les dernières, df.info() pour obtenir un résumé des types de données et des valeurs manquantes, et df.describe() pour des statistiques descriptives.

- 7/ Assigner le résultat d'une opération à une nouvelle variable crée une copie du DataFrame avec les modifications appliquées. Utiliser une méthode avec inplace=True modifie le DataFrame original sans créer de copie, ce qui peut être plus efficace en termes de mémoire.

- 8/ Pandas représente les valeurs manquantes avec l'objet `nan` (Not a Number) de `Numpy` pour les données numériques et avec None ou pd.NaT pour les dates/temps. Ces valeurs manquantes sont généralement ignorées dans les calculs de fonctions statistiques, ce qui peut affecter les résultats si elles ne sont pas traitées correctement.

- 9/ Concaténer consiste à assembler des DataFrames en les empilant verticalement ou en les alignant horizontalement, principalement utilisé lorsque les DataFrames ont le même schéma ou lorsque vous souhaitez empiler les données. Les jointures, inspirées des opérations JOIN en SQL, combinent les DataFrames sur la base de valeurs de clés communes et sont utilisées pour enrichir un ensemble de données avec des informations d'un autre ensemble.

</details>

:::

### Plusieurs manières de créer un DataFrame

Dans la cellule suivante, nous avons récupéré des données de caisses sur les ventes de différentes enseignes. Les données sont cependant présentées de deux manières différentes, dans un cas sous forme d'observations (chaque liste contient les données d'une ligne), dans l'autre sous forme de variables (chaque liste contient les données d'une colonne).

```{python}
data_list1 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

data_list2 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]
```

L'objectif est de construire dans les deux cas un même DataFrame qui contient chacune des 6 observations et des 4 variables, avec les mêmes noms dans les deux DataFrame. A chaque cas va correspondre une structure de données plus adaptée en entrée, dictionnaire ou liste de listes... faîtes le bon choix ! On vérifiera que les deux DataFrames sont identiques à l'aide de la méthode [equals()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.equals.html).

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
data_list1 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]

data_list2 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

# Si les données sont sous forme de colonnes : à partir d'un dictionnaire
data_dict = {
    'enseigne': data_list1[0],
    'produit': data_list1[1],
    'quantite': data_list1[2],
    'prix': data_list1[3]
}

df_from_dict = pd.DataFrame(data_dict)

# Si les données sont sous forme de lignes : à partir d'une liste de listes
columns = ['enseigne', 'produit', 'quantite', 'prix']
df_from_list = pd.DataFrame(data_list2, columns=columns)

# Vérification
df_from_dict.equals(df_from_list)
```

</details>

:::

### Sélection de données dans un DataFrame

Un DataFrame Pandas est créé avec des données de caisse (mêmes données que l'exercice précédent).

```{python}
data = {
    'enseigne': ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    'produit': ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    'quantite': [3, 2, 7, 5, 10, 1],
    'prix': [1.50, 2.30, 0.99, 5.00, 1.20, 3.10],
    'date_heure': pd.to_datetime(["2022-01-01 14:05", "2022-01-02 09:30", 
                                  "2022-01-03 17:45", "2022-01-04 08:20", 
                                  "2022-01-05 19:00", "2022-01-06 16:30"])
}

df = pd.DataFrame(data)
```

Utilisez les méthodes `loc` et `iloc` pour sélectionner des données spécifiques :

- Sélectionner les données de la première ligne.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.iloc[0])
```

</details>

:::

- Sélectionner toutes les données de la colonne "prix".

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[:, 'prix'])
```

</details>

:::

- Sélectionner les lignes correspondant à l'enseigne "Carrefour" uniquement.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[df['enseigne'] == 'Carrefour'])
```

</details>

:::

- Sélectionner les quantités achetées pour les produits classifiés "01.1.1" (Pain).

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[df['produit'] == '01.1.1', 'quantite'])
```

</details>

:::

- Sélectionner les données des colonnes "enseigne" et "prix" pour toutes les lignes.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[:, ['enseigne', 'prix']])
```

</details>

:::

- Sélectionner les lignes où la quantité achetée est supérieure à 5.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[df['quantite'] > 5])
```

</details>

:::

- Filtrer pour sélectionner toutes les transactions qui ont eu lieu après 15h.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[df['date_heure'].dt.hour > 15])
```

</details>

:::

- Sélectionner les transactions qui ont eu lieu le "2022-01-03".

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df.loc[df['date_heure'].dt.date == pd.to_datetime('2022-01-03').date()])
```

</details>

:::

### Exploration du fichier des prénoms

Le fichier des prénoms contient des données sur les prénoms attribués aux enfants nés en France entre 1900 et 2021. Ces données sont disponibles au niveau France, par département et par région, à l'adresse suivante : [https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262). L'objectif de ce tutoriel est de proposer une analyse de ce fichier, du nettoyage des données au statistiques sur les prénoms.

#### Partie 1 : Import et exploration des données


- Importez les données dans un DataFrame en utilisant cette [URL](https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip).
- Visualisez un échantillon des données. Repérez-vous d'éventuelles anomalies ?
- Affichez les principales informations du DataFrame. Repérez d'éventuelles variables dont le type serait incorrect, ou bien d'éventuelles valeurs manquantes.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_prenoms = pd.read_csv(url, sep=";")

df_prenoms.head(10)
df_prenoms.sample(n=50)

df_prenoms.info()
```

</details>

:::

#### Partie 2 : Nettoyage des données


- L'output de la méthode `info()` suggère des valeurs manquantes dans la colonne des prénoms. Affichez ces lignes. Vérifiez que ces valeurs manquantes sont correctement spécifiées.
- L'output de méthode `head()` montre une modalité récurrente "_PRENOMS_RARES" dans la colonne des prénoms. Quelle proportion des individus de la base cela concerne-t-il ? Convertir ces valeurs en `np.nan`.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
print(df_prenoms[df_prenoms["preusuel"].isna()])
prop_rares = df_prenoms.groupby("preusuel")["nombre"].sum()["_PRENOMS_RARES"] / df_prenoms["nombre"].sum()
print(prop_rares)  # ~ 2 % de la base
df_prenoms = df_prenoms.replace('_PRENOMS_RARES', np.nan)
```

</details>

:::

- On remarque que les prénoms de personnes dont l'année de naissance n'est pas connue sont regroupés sous la modalité `XXXX`. Quelle proportion des individus de la base cela concerne-t-il ? Convertir ces valeurs en `np.nan`.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
prop_xxxx = df_prenoms.groupby("annais")["nombre"].sum()["XXXX"] / df_prenoms["nombre"].sum()
print(prop_xxxx)  # ~ 1 % de la base
df_prenoms = df_prenoms.replace('XXXX', np.nan)
```

</details>

:::

- Supprimer les lignes contenant des valeurs manquantes de l'échantillon.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_prenoms = df_prenoms.dropna()
```

</details>

:::

- Convertissez la colonne `annais` en type numérique et la colonne `sexe` en type catégoriel.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_prenoms['annais'] = pd.to_numeric(df_prenoms['annais'])
df_prenoms['sexe'] = df_prenoms['sexe'].astype('category')
```

</details>

:::

- Vérifiez avec la méthode `info()` que le nettoyage a été correctement appliqué.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_prenoms.info()
```

</details>

:::

#### Partie 3 : Statistiques descriptives sur les naissances


- La [documentation](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262#documentation) du fichier nous informe qu'on peut considérer les données comme quasi-exhaustives à partir de 1946. Pour cette partie seulement, filtrer les données pour ne conserver que les données ultérieures.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_prenoms_post_1946 = df_prenoms[df_prenoms["annais"] >= 1946]
```

</details>

:::

- Calculez le nombre total de naissances par sexe.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
births_per_sex = df_prenoms_post_1946.groupby('sexe')['nombre'].sum()
print(births_per_sex)
```

</details>

:::

- Identifiez les cinq années ayant le plus grand nombre de naissances.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
top5_years = df_prenoms_post_1946.groupby('annais')['nombre'].sum().nlargest(5)
print(top5_years)
```

</details>

:::

#### Partie 4 : Analyse des prénoms


- Identifiez le nombre total de prénoms uniques dans le DataFrame.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
total_unique_names = df_prenoms['preusuel'].nunique()
print(total_unique_names)
```

</details>

:::

- Compter le nombre de personnes possédant un prénom d'une seule lettre.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
single_letter_names = df_prenoms[df_prenoms['preusuel'].str.len() == 1]['nombre'].sum()
print(single_letter_names)
```

</details>

:::

- Créez une "fonction de popularité" qui, pour un prénom donné, affiche l'année où il a été le plus donné ainsi que le nombre de fois où il a été donné cette année-là.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
def popularite_par_annee(df, prenom):
    # Filtrer le DataFrame pour ne garder que les lignes correspondant au prénom donné
    df_prenom = df[df['preusuel'] == prenom]

    # Grouper par année, sommer les naissances et identifier l'année avec le maximum de naissances
    df_agg = df_prenom.groupby('annais')['nombre'].sum()
    annee_max = df_agg.idxmax()
    n_max = df_agg[annee_max]

    print(f"Le prénom '{prenom}' a été le plus donné en {annee_max}, avec {n_max} naissances.")

# Test de la fonction avec un exemple
popularite_par_annee(df_prenoms, 'ALFRED')
```

</details>

:::

- Créez une fonction qui, pour un sexe donné, renvoie un DataFrame contenant le prénom le plus donné pour chaque décennie.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
def popularite_par_decennie(df, sexe):
    # Filtrage sur le sexe
    df_sub = df[df["sexe"] == sexe]

    # Calcul de la variable décennie
    df_sub["decennie"] = (df_sub["annais"] // 10) * 10

    # Calculer la somme des naissances pour chaque prénom et chaque décennie
    df_counts_decennie = df_sub.groupby(["preusuel", "decennie"])["nombre"].sum().reset_index()

    # Trouver l'indice du prénom le plus fréquent pour chaque décennie
    idx = df_counts_decennie.groupby("decennie")["nombre"].idxmax()

    # Utiliser l'indice pour obtenir les lignes correspondantes du DataFrame df_counts_decennie
    df_popularite_decennie = df_counts_decennie.loc[idx].set_index("decennie")

    return df_popularite_decennie

# Test de la fonction avec un exemple
popularite_par_decennie(df_prenoms, sexe=2)
```

</details>

:::

### Calcul d'une empreinte carbone par habitant au niveau communal

L'objectif de cet exercice est de calculer une empreinte carbone par habitant au niveau communal. Pour cela, il va falloir combiner deux sources de données : 

- les populations légales au niveau des communes, issues du recensement de la population ([source](https://www.insee.fr/fr/statistiques/6683037))

- les émissions de gaz à effet de serre estimées au niveau communal par l’ADEME ([source](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_))

Cet exercice constitue une version simplifiée d'un [TP complet pour la pratique de Pandas](https://pythonds.linogaliana.fr/content/manipulation/02b_pandas_TP.html#importer-les-donn%C3%A9es) proposé par Lino Galiana dans son [cours à l'ENSAE](https://pythonds.linogaliana.fr/). 

#### Partie 1 : Exploration des données sur les populations légales communales

- Importez le fichier CSV `communes.csv`.
- Utilisez les méthodes `.sample()`, `.info()` et `.describe()` pour obtenir un aperçu des données.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_pop_communes = pd.read_csv("data/communes.csv", sep=";")

df_pop_communes.sample(10)
df_pop_communes.info()
df_pop_communes.describe()
```

</details>

:::

- Identifiez et retirez les lignes correspondant aux communes sans population.
- Supprimez les colonnes "PMUN" et "PCAP", non pertinentes pour l'analyse.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
n_communes_0_pop = df_pop_communes[df_pop_communes["PTOT"] == 0].shape[0]
print(n_communes_0_pop)
df_pop_communes = df_pop_communes[df_pop_communes["PTOT"] > 0]

df_pop_communes = df_pop_communes.drop(columns=["PMUN", "PCAP"])
```

</details>

:::

Les communes qui ont les noms les plus longs sont-elles aussi les communes les moins peuplées ? Pour le savoir :
- Créez une nouvelle variable qui contient le nombre de caractères de chaque commune à l'aide de la méthode [str.len()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html)
- Calculez la corrélation entre cette variable et la population totale avec la méthode [corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_pop_communes_stats = df_pop_communes.copy()
df_pop_communes_stats['longueur'] = df_pop_communes_stats['COM'].str.len()
df_pop_communes_stats['longueur'].corr(df_pop_communes_stats['PTOT'])
```

</details>

:::

#### Partie 2 : Exploration des données sur les émissions communales

- Importez les données d'émission depuis cette [URL](https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv)
- Utilisez les méthodes `.sample()`, `.info()` et `.describe()` pour obtenir un aperçu des données.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
url_ademe = "https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv"
df_emissions = pd.read_csv(url_ademe)

df_emissions.sample(10)
df_emissions.info()
df_emissions.describe()
```

</details>

:::

- Y a-t-il des lignes avec des valeurs manquantes pour toutes les colonnes d'émission ? Vérifiez-le à l'aide des méthodes [isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) et [all()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.all.html).

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_emissions_num = df_emissions.select_dtypes(['number'])
only_nan = df_emissions_num[df_emissions_num.isnull().all(axis=1)]
only_nan.shape[0]
```

</details>

:::

- Créez une nouvelle colonne qui donne les émissions totales par commune
- Afficher les 10 communes les plus émettrices. Qu'observez-vous dans les résultats ?

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_emissions['emissions_totales'] = df_emissions.sum(axis = 1, numeric_only = True)

df_emissions.sort_values(by="emissions_totales", ascending=False).head(10)
```

</details>

:::

- Il semble que les postes majeurs d'émissions soient "Industrie hors-énergie" et "Autres transports international". Pour vérifier si cette conjecture tient, calculer la corrélation entre les émissions totales et les postes sectoriels d'émissions à l'aide de la méthode [corrwith()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html).

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_emissions.corrwith(df_emissions["emissions_totales"], numeric_only=True)
```

</details>

:::

- Extraire du code commune le numéro de département dans une nouvelle variable
- Calculer les émissions totales par département
- Afficher les 10 principaux départements émetteurs. Les résultats sont-ils logiques par rapport à l'analyse au niveau communal ? 

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_emissions["dep"] = df_emissions["INSEE commune"].str[:2]
df_emissions.groupby("dep").agg({"emissions_totales": "sum"}).sort_values(by="emissions_totales", ascending=False).head(10)
```

</details>

:::

#### Partie 3 : Vérifications préalables pour la jointure des sources de données

Pour effectuer une jointure, il est toujours préférable d'avoir une clé de jointure, i.e. une colonne commune aux deux sources, qui identifie uniquement les unités statistiques. L'objet de cette partie est de trouver la clé de jointure pertinente.

- Vérifiez si la variable contenant les noms de commune contient des doublons

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
doublons = df_pop_communes.groupby('COM').count()['DEPCOM']
doublons = doublons[doublons>1]
doublons = doublons.reset_index()
doublons
```

</details>

:::

- Filtrez dans le DataFrame initial les communes dont le nom est dupliqué, et triez-le par code commune. Les doublons semblent-ils problématiques ?

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_pop_communes_doublons = df_pop_communes[df_pop_communes["COM"].isin(doublons["COM"])]
df_pop_communes_doublons.sort_values('COM')
```

</details>

:::

- Vérifiez que les codes commune identifient de manière unique la commune associée

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
(df_pop_communes_doublons.groupby("DEPCOM")["COM"].nunique() != 1).sum()
```

</details>

:::

- Affichez les communes présentes dans les données communales mais pas dans les données d'émissions, et inversement. Qu'en concluez-vous ?

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
## Observations qui sont dans les pop légales mais pas dans les données d'émissions
df_pop_communes[~df_pop_communes["DEPCOM"].isin(df_emissions["INSEE commune"])]

## Observations qui sont dans les données d'émissions mais pas dans les pop légales
df_emissions[~df_emissions["INSEE commune"].isin(df_pop_communes["DEPCOM"])]
```

</details>

:::

#### Partie 4 : Calcul d'une empreinte carbone par habitant pour chaque commune

- Joindre les deux DataFrames à l'aide de la fonction à partir du code commune. Attention : les variables ne s'appellent pas de la même manière des deux côtés !

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_emissions_pop = pd.merge(df_pop_communes, df_emissions, how="inner", left_on="DEPCOM", right_on="INSEE commune")
df_emissions_pop
```

</details>

:::

- Calculer une empreinte carbone pour chaque commune, correspondant aux émissions totales de la commune divisées par sa population totale.
- Affichez les 10 communes avec les empreintes carbones les plus élevées. 
- Les résultats sont-ils identiques à ceux avec les émissions totales ? Qu'en concluez-vous ?

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_emissions_pop["empreinte_carbone"] = df_emissions_pop["emissions_totales"] / df_emissions_pop["PTOT"]
df_emissions_pop.sort_values("empreinte_carbone", ascending=False).head(10)
```

</details>

:::

### Analyse de l'évolution d'un indice de production

Vous avez à disposition dans le dossier `data/` deux jeux de données CSV :
- `serie_glaces_valeurs.csv` contient les valeurs mensuelles de l'indice de prix de production de l'industrie française des glaces et sorbets
- `serie_glaces_metadonnees.csv` contient les métadonnées associées, notamment les codes indiquant le statut des données.

L'objectif est d'utiliser `Pandas` pour calculer :
- l'évolution de l'indice entre chaque période (mois)
- l'évolution de l'indice en glissement annuel (entre un mois donné et le même mois l'année suivante).

#### Partie 1 : Import des données

- Importez les deux fichiers CSV dans des DataFrames. Attention, dans les deux cas, il y a des lignes superflues avant les données, qu'il faudra sauter à l'aide du paramètre `skiprows` de la fonction [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).
- Donnez des noms simples et pertinents aux différentes variables.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_valeurs = pd.read_csv('data/serie_glaces_valeurs.csv', delimiter=';',
                         skiprows=4, names=["periode", "indice", "code"])
df_metadata = pd.read_csv('data/serie_glaces_metadonnees.csv', delimiter=';',
                          skiprows=5, names=["code", "signification"])
```

</details>

:::

#### Partie 2 : Filtrage des données pertinentes

- Fusionner les deux DataFrames afin de récupérer la signification des codes présents dans les données.
- Filtrer les données de sorte à ne conserver que les données de type "Valeur normale".
- Supprimer les colonnes liées aux codes, dont nous n'avons plus besoin pour la suite.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_merged = pd.merge(df_valeurs, df_metadata, how='left', on='code')

df_clean = df_merged[df_merged['code'] == "A"]
df_clean = df_clean[["periode", "indice"]]
```

</details>

:::

#### Partie 3 : Pré-traitement des données

Vérifiez si les types des variables sont pertinents selon leur nature. Sinon, convertissez-les avec les fonctions idoines.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_clean.info()
df_clean['periode'] = pd.to_datetime(df_clean['periode'])
df_clean['indice'] = pd.to_numeric(df_clean['indice'])
df_clean.info()
```

</details>

:::

#### Partie 4 : Calcul de l'évolution périodique

- Utilisez la méthode [shift()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) pour créer une nouvelle colonne qui contiendra l'indice du trimestre précédent
- Calculez la différence entre l'indice actuel et l'indice décalé pour obtenir l'évolution (en pourcentage) d'un trimestre à l'autre

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_clean['indice_prec'] = df_clean['indice'].shift(1)
df_clean['evo'] = ((df_clean['indice'] - df_clean['indice_prec']) / df_clean['indice_prec']) * 100

## Méthode alternative
df_clean['evo_alt'] = df_clean['indice'].pct_change(periods=1) * 100
```

</details>

:::

#### Partie 5 : Calcul de l'évolution glissante sur 12 mois

Comme vous avez pu le voir dans la solution de l'exercice précédent, la méthode [pct_change()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html) permet précisément de calculer une évolution entre deux périodes. Utiliser cette méthode pour calculer une évolution (en pourcentage) en glissement annuel pour chaque mois.

```{python}
# Testez votre réponse dans cette cellule

```

::: {.cell .markdown}

<details>
<summary>Afficher la solution</summary>

```python
df_clean["evo_glissement_annuel"] = df_clean['indice'].pct_change(periods=12) * 100
df_clean.head(20)
```

</details>

:::

:::













::: {.content-visible when-profile="en"}

---
title: "Working with tabular data using Pandas"
---

Statistical analysis is generally based on **tabular data**, where each row represents an observation and each column a variable. To handle this type of data and easily apply standard data analysis methods, dedicated objects have been designed: `DataFrames`. Users of `R` are well acquainted with this data structure, which is native to this statistics-oriented language. In `Python`, a general-purpose language, this object does not exist natively. Fortunately, a very comprehensive and convenient library, designed as an overlay to `NumPy`, introduces the `DataFrame` object in `Python` and allows for simple and intuitive data manipulation and analysis: `Pandas`.

::: {.callout-note}
Pandas is the central element of the data science ecosystem in Python, offering virtually infinite data processing capabilities. Moreover, there are generally multiple ways to perform the same operation in Pandas. Consequently, this chapter is particularly long and dense with new features. The goal is not to memorize all the methods presented throughout this chapter, but rather to have a general overview of what is possible in order to use the right tools in projects. In particular, the end-of-chapter exercises and the mini-projects at the end of the course will provide an opportunity to apply this new knowledge to concrete problems.
:::

Let's start by importing the `Pandas` library. The common usage is to give it the alias `pd` to simplify future calls to the package's objects and functions. We also import `NumPy` as we will compare the fundamental objects of the two packages.

```{python}
import pandas as pd
import numpy as np
```

## Data structures

To fully understand how `Pandas` works, it is important to focus on its fundamental objects. We will therefore first study the `Series`, whose concatenation allows us to build a `DataFrame`.

### The `Series`

A Series is a one-dimensional data container that can hold any type of data (integers, strings, Python objects...). However, a Series is of a given type: a Series containing only integers will be of type `int`, and a Series containing objects of different natures will be of type `object`. Let's build our first Series from a list to check this behavior.

```{python}
l = [1, "X", 3]
s = pd.Series(l)
print(s)
```

In particular, we can access the data of a Series by position, as for a list or an array.

```{python}
print(s[1])
```

At first glance, we do not see much difference between a Series and a one-dimensional `NumPy` array. However, there is a significant difference: the presence of an index. The observations have an associated label. When we create a Series without specifying anything, the index is automatically set to the integers from 0 to n-1 (with n being the number of elements in the Series). But it is possible to pass a specific index (e.g., dates, town names, etc.).

```{python}
s = pd.Series(l, index=["a", "b", "c"])
print(s)
```

This allows us to access the data by label:

```{python}
s["b"]
```

This difference may seem minor at first, but it becomes essential for constructing the DataFrame. For the rest, Series behave very similarly to NumPy arrays: calculations are vectorized, we can directly sum two Series, etc. Moreover, we can easily convert a Series into an array via the `values` attribute. This naturally loses the index...

```{python}
s = pd.Series(l, index=["a", "b", "c"])
s.values
```

### The `DataFrame`

Fundamentally, a DataFrame consists of a collection of Series, aligned by their indexes. This concatenation thus constructs a data table, with Series corresponding to columns, and the index identifying the rows. The following figure ([source](https://www.geeksforgeeks.org/creating-a-pandas-dataframe/)) helps to understand this data structure.

![](img/structure_df.png){fig-align="center" width="800"}

A DataFrame can be constructed in multiple ways. In practice, we generally build a DataFrame directly from tabular data files (e.g., CSV, Excel), rarely by hand. So, we will only illustrate the most common manual construction method: from a data dictionary.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df
```

A Pandas DataFrame has a set of useful attributes that we will discover throughout this tutorial. For now, let's focus on the most basic ones: the index and the column names. By default, the index is initialized, as for Series, to the list of positions of the observations. We could have specified an alternative index when constructing the DataFrame by specifying the `index` argument of the `pd.DataFrame` function.

```{python}
df.index
```

```{python}
df.columns
```

Often, rather than specifying an index manually during the construction of the DataFrame, we will want to use a certain column of the DataFrame as an index. We use the `set_index` method associated with DataFrames for this.

```{python}
df = df.set_index("date")
df
```

The index attribute has naturally changed:

```{python}
df.index
```

## Selecting data

When manipulating tabular data, it is common to want to extract specific columns from a `DataFrame`. This extraction is simple with `Pandas` using square brackets.

### Selecting columns

#### Selecting a single column

To extract a single column, we can use the following syntax:

```{python}
selected_column = df["var1"]
selected_column
```

The `selected_column` object here returns the column named `var1` from the `DataFrame` `df`. But what type is this object? To answer this question, we use the `type()` function:

```{python}
type(selected_column)
```

As we can see, the result is a `Series`, which is a one-dimensional object in `Pandas`.

Another useful attribute to know is `shape`. It allows us to know the dimension of the object. For a `Series`, `shape` will return a tuple whose first element indicates the number of rows.

```{python}
selected_column.shape
```

#### Selecting multiple columns

To extract multiple columns, just pass a list of the desired column names:

```{python}
selected_columns = df[["var1", "var2", "experiment"]]
selected_columns
```

This snippet shows the columns `var1`, `var2`, and `experiment` from the `DataFrame` `df`. Let's now check its type:

```{python}
type(selected_columns)
```

The result is a `DataFrame` because it is a two-dimensional object. We can also check its shape with the `shape` attribute. In this case, the tuple returned by `shape` will contain two elements: the number of rows and the number of columns.

```{python}
selected_columns.shape
```

### Selecting rows

#### Using `loc` and `iloc`

When we want to select specific rows in a DataFrame, we can use two main methods: `loc` and `iloc`.

- `iloc` allows selecting rows and columns by their position, i.e., by numeric indices.

Example, selecting the first 3 rows:

```{python}
df.iloc[0:3, :]
```

- `loc` works with labels. If the DataFrame's indexes are numbers, they resemble positions, but this is not necessarily the case. It is crucial to note that, unlike `iloc`, with `loc`, the end index is included in the selection.

```{python}
df.loc["2022-01-01":"2022-01-03", :]
```

#### Filtering data based on conditions

In practice, rather than selecting rows based on positions or labels, we often want to filter a DataFrame based on certain conditions. In this case, we primarily use boolean filters.

- **Inequalities**: We might want to keep only the rows that meet a certain condition.

Example, filtering rows where the value of the `var2` column is greater than 0:

```{python}
df[df['var2'] >= 0]
```

- **Membership with `isin`**: If we want to filter data based on a list of possible values, the `isin` method is very useful.

Example, to keep only the rows where the `experiment` column has values 'test' or 'validation':

```{python}
df[df['experiment'].isin(['train', 'validation'])]
```

These methods can be combined to create more complex conditions. It is also possible to use logical operators (`&` for "and", `|` for "or") to combine multiple conditions. Be careful to enclose each condition in parentheses when combining them.

Example, selecting rows where `var2` is greater than 0 and `experiment` is equal to 'test' or 'validation':

```{python}
df[(df['var2'] >= 0) & (df['experiment'].isin(['train', 'validation']))]
```

## Exploring tabular data

In public statistics, the starting point is generally not the manual generation of data but rather pre-existing tab

ular files. These files, whether from surveys, administrative databases, or other sources, constitute the raw material for any subsequent analysis. Pandas offers powerful tools to import these tabular files and explore them for further manipulations.

### Importing and exporting data

#### Importing a CSV file

As we saw in a previous lab, the CSV format is one of the most common formats for storing tabular data. We previously used the `csv` library to handle them as text files, but it was not very convenient. To recall, the syntax for reading a CSV file and displaying the first lines was as follows:

```{python}
import csv

rows = []

with open("data/departement2021.csv") as file_in:
    csv_reader = csv.reader(file_in)
    for row in csv_reader:
        rows.append(row)

rows[:5]
```

With Pandas, just use the `read_csv()` function to import the file as a DataFrame, then the `head()` function.

```{python}
df_departements = pd.read_csv('data/departement2021.csv')
df_departements.head()
```

It is also possible to import a CSV file directly from a URL. This is particularly convenient when the data is regularly updated on a website, and we want to access the latest version without manually downloading the file each time. Let's take the example of a CSV file available on the INSEE website: the file of given names, from civil status data. We also note another handy feature: the CSV file is compressed (in `zip` format), but Pandas can recognize and decompress it before importing.

```{python}
# Importing a CSV file from a URL
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_prenoms_url = pd.read_csv(url, sep=";")
df_prenoms_url.head()
```

When working with CSV files, there are many optional arguments available in the `read_csv()` function that allow us to adjust the import process according to the specifics of the file. These arguments can, for example, define a specific delimiter (as above for the given names file), skip certain lines at the beginning of the file, or define data types for each column, and many others. All these parameters and their usage are detailed in the [official documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

#### Exporting to CSV format

Once the data has been processed and modified within Pandas, it is common to want to export the result as a CSV file for sharing, archiving, or use in other tools. Pandas offers a simple method for this operation: `to_csv()`. Suppose we want to export the data from the `df_departements` DataFrame specific to the five overseas departments.

```{python}
df_departements_dom = df_departements[df_departements["DEP"].isin(["971", "972", "973", "974", "975"])]
df_departements_dom.to_csv('output/departements2021_dom.csv')
```

One of the key arguments of the `to_csv()` method is `index`. By default, `index=True`, which means that the DataFrame's index will also be written in the CSV file. We can verify this by printing the first lines of our CSV file: Pandas has added an unnamed column, which contains the index of the retained rows.

```{python}
with open("output/departements2021_dom.csv") as file_in:
    for i in range(5):
        row = next(file_in).strip()
        print(row)
```

In some cases, notably when the index does not provide useful information or is simply automatically generated by Pandas, we might want to exclude it from the exported file. To do this, we can set `index=False`.

```{python}
df_departements_dom.to_csv('output/departements2021_dom_noindex.csv', index=False)
```

#### Importing a Parquet file

The Parquet format is another format for storing tabular data, increasingly used. Without going into technical details, the Parquet format has various characteristics that make it a preferred choice for storing and processing large volumes of data. Due to these advantages, this format is increasingly used for data dissemination at INSEE. It is therefore essential to know how to import and query Parquet files with Pandas.

Importing a Parquet file into a Pandas DataFrame is as easy as for a CSV file. The function is called `read_parquet()`.

```{python}
df_departements = pd.read_parquet('data/departement2021.parquet')
df_departements.head()
```

#### Exporting to Parquet format

Again, everything works as in the CSV world: we use the `to_parquet()` method to export a DataFrame to a Parquet file. Similarly, we can choose to export or not the index, using the `index` parameter (which defaults to `True`).

```{python}
df_departements_dom = df_departements[df_departements["DEP"].isin(["971", "972", "973", "974", "975"])]
df_departements_dom.to_parquet('output/departements2021_dom.parquet', index=False)
```

One of the major strengths of the Parquet format, compared to text formats like CSV, is its ability to store metadata, i.e., data that helps better understand the data contained in the file. In particular, a Parquet file includes in its metadata the data schema (variable names, variable types, etc.), making it a very suitable format for data dissemination. Let's verify this behavior by revisiting the DataFrame we defined earlier.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df = df.assign(
    experiment=pd.Categorical(df["experiment"]),
    date=pd.to_datetime(df["date"])
)
```

This time, we use two specific data types, for categorical data (`category`) and for temporal data (`datetime`). We will see later in the tutorial how to use these types. For now, let's simply note that Pandas stores these types in the data schema.

```{python}
df.info()
```

Let's now verify that exporting and re-importing this data in Parquet preserves the schema.

```{python}
df.to_parquet("output/df_test_schema.parquet", index=False)
df_test_schema_parquet = pd.read_parquet('output/df_test_schema.parquet')

df_test_schema_parquet.info()
```

Conversely, a CSV file, which by definition only contains text, does not preserve this data. The variables for which we specified the type are imported as strings (type `object` in Pandas).

```{python}
df.to_csv("output/df_test_schema.csv", index=False)
df_test_schema_csv = pd.read_csv('output/df_test_schema.csv')

df_test_schema_csv.info()
```

### Viewing a sample of data

When working with large datasets, it is often useful to quickly view a sample of the data to get an idea of its structure, format, or even to detect potential problems. Pandas offers several methods for this.

The `head()` method displays the first rows of the DataFrame. By default, it returns the first 5 rows, but we can specify another number as an argument if necessary.

```{python}
df_departements.head()
```

```{python}
df_departements.head(10)
```

Conversely, the `tail()` method gives a preview of the last rows of the DataFrame.

```{python}
df_departements.tail()
```

Displaying the first or last rows may sometimes not be representative of the entire dataset, especially when the data is sorted. To minimize the risk of obtaining a biased overview of the data, we can use the `sample()` method, which selects a random sample of rows. By default, it returns a single row, but we can request a specific number of rows using the `n` argument.

```{python}
df_departements.sample(n=5)
```

### Getting an overview of the data

One of the first steps when exploring new data is to understand the general structure of the dataset. The `info()` method in Pandas provides a quick overview of the data, including data types, the presence of missing values, and memory usage.

```{python}
df.info()
```

Several key pieces of information can be extracted from this result:

- **index**: The DataFrame has a `RangeIndex`, which means the index is a simple numeric sequence. Here, the index ranges from 0 to 5, for a total of 6 entries.

- **schema**: The list of columns is displayed with very useful information about the data schema:

  - **Non-Null Count**: The number of **non-missing** (non-`nan`) values in the column. If this number is less than the total number of entries (in our case, 6), it means the column contains missing values. Note the possible ambiguity on "null": this indeed means missing values, not values equal to 0. Thus, in our case, the number of "non-null" values for the `var1` variable is 5.

  - **Dtype**: The data type of the column, which helps understand the nature of the information stored in each column. For example, `float64`

 (real numbers), `int32` (integers), `category` (categorical variable), `datetime64[ns]` (temporal information), and `object` (text or mixed data).

Using `info()` is a quick and effective way to get an overview of a DataFrame, quickly identify columns containing missing values, and understand the data structure.

### Calculating descriptive statistics

In addition to the information returned by the `info()` method, we might want to obtain simple descriptive statistics to quickly visualize the distributions of variables. The `describe()` method provides a synthetic view of the distribution of data in each column.

```{python}
df.describe()
```

It should be noted that `describe()` only returns statistics for numeric columns by default. If we want to include columns of other types, we need to specify this via the `include` argument. For example, `df.describe(include='all')` will return statistics for all columns, including metrics such as the unique count, the most frequent value, and the frequency of the most frequent value for non-numeric columns.

```{python}
df.describe(include='all')
```

Note again that the `count` variable returns the number of **non-missing** values in each variable.

## Main data manipulations

### Transforming data

Data transformation operations are essential for shaping, cleaning, and preparing data for analysis. Transformations can apply to the entire DataFrame, specific columns, or specific rows.

#### Transforming a DataFrame

To transform an entire DataFrame (or a sub-DataFrame), it is possible to use vectorized functions, which allow quickly applying an operation to all elements of the DataFrame. This includes a number of methods available for `Series`, as well as NumPy mathematical functions, etc.

For example, raising each numeric value in a DataFrame to the power of 2:

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
    }
)

df ** 2
```

or taking the absolute value:

```{python}
np.abs(df)
```

Some methods available for `Series` can also be used to transform an entire DataFrame. For example, the very useful [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) method, which allows replacing all occurrences of a given value with another value. For example, suppose the value 0 in the `var1` column actually indicates a measurement error. It would be preferable to replace it with a missing value.

```{python}
df.replace(0, np.nan)
```

::: {.callout-warning title="Assignment or in-place methods?"}
In the previous example, applying the [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) method does not directly modify the DataFrame. To make the modification persistent, one possibility is to assign the result to an object:

```{python}
df = df.replace(0, np.nan)
```

A second possibility is, when methods offer it, to use the `inplace` argument. When `inplace=True`, the operation is performed "in place", and the DataFrame is therefore directly modified.

```{python}
df.replace(0, np.nan, inplace=True)
```

In practice, it is better to limit `inplace` operations. They do not favor the reproducibility of analyses, as the re-execution of the same cell will give different results each time.
:::

#### Transforming columns

In some cases, we will not want to apply transformations to the entire data but to specific variables. Transformations possible at the DataFrame level (vectorized functions, methods like [replace()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html), etc.) naturally remain possible at the column level.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
    }
)

np.abs(df["var2"])
```

```{python}
df["var1"].replace(0, np.nan)
```

But there are other transformations that are generally applied at the level of one or a few columns. For example, when the schema has not been properly recognized upon import, it may happen that numeric variables are defined as strings (type `object` in Pandas).

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan],
        "var2": ["1", "5", "18"],
    }
)

df.info()
```

In this case, we can use the `astype` method to convert the column to the desired type.

```{python}
df['var2'] = df['var2'].astype(int)

df.info()
```

Another frequent operation is renaming one or more columns. To do this, we can use the [rename()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) method, passing a dictionary containing as many key-value pairs as variables to be renamed, where each key-value pair is of the form `'old_name': 'new_name'`.

```{python}
df.rename(columns={'var2': 'age'})
```

Finally, we might want to remove columns from the DataFrame that are not or no longer useful for analysis. For this, we use the [drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method, to which we pass either a string (name of a column if we want to remove only one) or a list of column names to remove.

```{python}
df.drop(columns=['var1'])
```

#### Transforming rows

In statistics, we generally apply transformations involving one or more columns. However, in some cases, it is necessary to apply transformations at the row level. For this, we can use the [apply()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html) method of Pandas, applied to the row axis (`axis=1`). Let's illustrate its operation with a simple case. First, we generate data.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
    }
)

df.head()
```

We now apply the `apply()` function to the DataFrame to calculate a new variable that is the sum of the two existing ones.

```{python}
df['sum_row'] = df.apply(lambda row: row['var1'] + row['var2'], axis=1)

df.head()
```

::: {.callout-tip title="Lambda functions"}
A `lambda` function is a small anonymous function. It can take any number of arguments but can have only one expression. In the example above, the `lambda` function takes a row as an argument and returns the sum of the `var1` and `var2` columns for that row.

Lambda functions allow defining simple functions "on the fly" without having to give them a name. In our example, this would have been perfectly equivalent to the following code:

```{python}
def sum_row(row):
    return row['var1'] + row['var2']

df['sum_row'] = df.apply(sum_row, axis=1)
```
:::

Although `apply()` offers great flexibility, it is not the most efficient method, especially for large datasets. Vectorized operations are always preferable as they process data in blocks rather than row by row. In our case, it would have been preferable to create our variable using column operations.

```{python}
df['sum_row_vect'] = df['var1'] + df['var2']

df.head()
```

However, we might find ourselves in certain (rare) cases where an operation cannot be easily vectorized or where the logic is complex. Suppose, for example, we want to combine the values of several columns based on certain conditions.

```{python}
def combine_columns(row):
    if row['var1'] > 6:
        return str(row['var2'])
    else:
        return str(row['var2']) + "_" + row['date']

df['combined_column'] = df.apply(combine_columns, axis=1)

df
```

### Sorting values

Sorting data is particularly useful for exploring and visualizing data. With Pandas, we use the [sort_values()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) method to sort the values of a DataFrame based on one or more columns.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04"],
    }
)

df
```

To sort the values based on a single column, just pass the column name as a parameter.

```{python}
df.sort_values(by='var1')
```

By default, the sorting is done in ascending

 order. To sort the values in descending order, just set `ascending=False`.

```{python}
df.sort_values(by='var1', ascending=False)
```

If we want to sort the DataFrame on multiple columns, we can provide a list of column names. We can also choose to sort in ascending order for some columns and descending order for others.

### Aggregating data

Aggregating data is a process where the data is broken down into groups based on certain criteria and then aggregated using an aggregation function applied independently to each group. This operation is common in exploratory analysis or when preprocessing data for visualization or statistical modeling.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "train", "train", "validation"],
        "date": ["2022-01-01", "2022-01-02", "2022-01-03", "2022-01-04", "2022-01-05", "2022-01-06"],
        "sample": "sample1"
    }
)

df.head()
```

#### The `groupBy` operation

The `groupBy` method in Pandas allows dividing the DataFrame into subsets based on the values of one or more columns, and then applying an aggregation function to each subset. It returns a `DataFrameGroupBy` object that is not very useful by itself but is an essential intermediate step to then apply one or more aggregation function(s) to the different groups.

```{python}
df.groupby('experiment')
```

#### Aggregation functions

Once the data is grouped, we can apply aggregation functions to obtain a statistical summary. Pandas includes a number of these functions, the complete list of which is detailed in the [documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-aggregation-methods). Here are some examples of using these methods.

For example, count the number of occurrences in each group.

```{python}
df.groupby('experiment').size()
```

Calculate the sum of a variable by group.

```{python}
df.groupby('experiment')['var1'].sum()
```

Or count the number of unique values of a variable by group. The possibilities are numerous.

```{python}
# For the number of unique values of 'var2' in each group
df.groupby('experiment')['var2'].nunique()
```

When we want to apply multiple aggregation functions at once or custom functions, we use the `agg` method. This method accepts a list of functions or a dictionary that associates column names with functions to apply. This allows for finer application of aggregation functions.

```{python}
df.groupby('experiment').agg({'var1': 'mean', 'var2': 'count'})
```

::: {.callout-note title="Method chaining"}
The previous examples illustrate an important concept in Pandas: method chaining. This term refers to the possibility of chaining transformations applied to a DataFrame by applying methods to it in a chain. At each applied method, an intermediate DataFrame is created (but not assigned to a variable), which becomes the input of the next method.

Method chaining allows combining several operations into a single code expression. This can improve efficiency by avoiding intermediate assignments and making the code more fluid and readable. It also favors a functional programming style where data flows smoothly through a chain of transformations.
:::

#### Effects on the index

It is interesting to note the effects of the aggregation process on the DataFrame's index. The last example above illustrates this well: the groups, i.e., the modalities of the variable used for aggregation, become the values of the index.

We might want to reuse this information in subsequent analyses and therefore want it as a column. For this, just reset the index with the `reset_index()` method.

```{python}
df_agg = df.groupby('experiment').agg({'var1': 'mean', 'var2': 'count'})
df_agg.reset_index()
```

### Handling missing values

Missing values are a common reality in real-world data processing and can occur for various reasons, such as non-responses to a questionnaire, data entry errors, data loss during transmission, or simply because the information is not applicable. Pandas offers several tools to handle missing values.

#### Representation of missing values

In Pandas, missing values are generally represented by `np.nan`, a special marker provided by the `NumPy` library. While it is preferable to use this object to denote missing values, note that the `None` object in `Python` is also understood as a missing value by `Pandas`.

Let's verify this property. To identify where the missing values are, we use the `isna()` function, which returns a boolean DataFrame indicating `True` where the values are `NaN`.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
        "sample": "sample1"
    }
)

df.isna()
```

#### Calculations on columns containing missing values

During statistical calculations, missing values are generally ignored. For example, the `.mean()` method calculates the mean of non-missing values.

```{python}
df['var1'].mean()
```

However, calculations involving multiple columns do not always ignore missing values and can often result in `NaN`.

```{python}
df['var3'] = df['var1'] + df['var2']

df
```

#### Removing missing values

The `dropna()` method allows us to remove rows (`axis=0`) or columns (`axis=1`) containing missing values. By default, any row containing at least one missing value is removed.

```{python}
df.dropna()
```

By changing the `axis` parameter, we can request that any column containing at least one missing value be removed.

```{python}
df.dropna(axis=1)
```

Finally, the `how` parameter defines the deletion mode. By default, a row or column is removed when at least one value is missing (`how=any`), but it is possible to remove the row/column only when all values are missing (`how=all`).

#### Replacing missing values

To handle missing values in a DataFrame, a common approach is imputation, which involves replacing the missing values with other values. The `fillna()` method allows us to perform this operation in various ways. One possibility is replacement by a constant value.

```{python}
df['var1'].fillna(value=0)
```

::: {.callout-warning title="Changing the representation of missing values"}
It can sometimes be tempting to change the manifestation of a missing value for visibility reasons, for example by replacing it with a string:

```{python}
df['var1'].fillna(value="MISSING")
```

In practice, this is not recommended. It is indeed preferable to stick to Pandas' standard convention (using `np.nan`), firstly for standardization purposes that facilitate reading and maintaining the code, but also because the standard convention is optimized for performance and calculations from data containing missing values.
:::

Another frequent imputation method is to use a statistical value, such as the mean or median of the variable.

```{python}
df['var1'].fillna(value=df['var1'].mean())
```

::: {.callout-warning title="Imputation bias"}
Replacing missing values with a constant value, such as zero, the mean, or the median, can be problematic. If the data is not missing at random (MNAR), this can introduce bias into the analysis. MNAR variables are variables whose probability of being missing is related to their own value or other variables in the data. In such cases, more sophisticated imputation may be necessary to minimize distortions. We will see an example in the end-of-tutorial exercise.
:::

### Handling data of specific types

#### Text data

Text data often requires cleaning and preparation before analysis. Pandas provides an array of vectorized operations via the `str` library that make preparing text data both simple and very efficient. Again, the possibilities are numerous and detailed in the [documentation](https://pandas.pydata.org/docs/user_guide/text.html). Here we present the most frequently used methods in data analysis.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", "test", "train", "validation"],
        "sample": ["  sample1", "sample1", "sample2", "   sample2   ", "sample2  ", "sample1"]
    }
)

df
```

A first frequent operation is to extract certain characters from a string. We use the `str[n:]` function (with a somewhat peculiar syntax) for this. For example, if we want to extract the last character of the `sample` variable to retain only the sample number.

```{python}
df["sample_n"] = df["sample"].str[-1:]

df
```

The principle was correct, but the presence of extraneous spaces in our text data (which were not visible when viewing the DataFrame!) made the operation more difficult than expected. This is an opportunity to introduce the `strip` family of methods (`.str.strip()`, `.str.lstrip()`, and `.str.rstrip()`) that respectively remove extr

aneous spaces from both sides or one side.

```{python}
df["sample"] = df["sample"].str.strip()
df["sample_n"] = df["sample"].str[-1:]

df
```

We might also want to filter a DataFrame based on the presence or absence of a certain string (or substring) of characters. For this, we use the `.str.contains()` method.

```{python}
df[df['experiment'].str.contains('test')]
```

Finally, we might want to replace a string (or substring) of characters with another, which the `str.replace()` method allows.

```{python}
df['experiment'] = df['experiment'].str.replace('validation', 'val')

df
```

#### Categorical data

Categorical data is variables that contain a limited number of categories. Similar to `R` with the notion of `factor`, Pandas has a special data type, `category`, which is useful for representing categorical data more efficiently and informatively. Categorical data is indeed optimized for certain types of data and can speed up operations like grouping and sorting. It is also useful for visualization, ensuring that categories are displayed in a coherent and logical order.

To convert a variable to the `category` format, we use the `astype()` method.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1.3, 5.6, np.nan, np.nan, 0, np.nan],
        "var2": np.random.randint(-10, 10, 6),
        "experiment": ["test", "train", "test", None, "train", "validation"],
    }
)
print(df.dtypes)
```

```{python}
df['experiment'] = df['experiment'].astype('category')

print(df.dtypes)
```

This conversion gives us access to some very useful methods, specific to handling categorical variables. For example, it can be useful to rename categories for clarity or standardization.

```{python}
df['experiment'] = df['experiment'].cat.rename_categories({'test': 'Test', 'train': 'Train', 'validation': 'Validation'})
df
```

Sometimes, the order of categories is significant, and we might want to modify it. This is particularly important for visualization, as the categories will by default be displayed in the specified order.

```{python}
df_cat = df['experiment'].cat.reorder_categories(['Test', 'Train', 'Validation'], ordered=True)
df.groupby("experiment").mean().plot(kind='bar')
```

#### Temporal data

Temporal data is often present in tabular data to temporally identify the observations collected. Pandas offers functionalities for handling these types of data, particularly through the `datetime64` type, which allows precise manipulation of dates and times.

```{python}
df = pd.DataFrame(
    data = {
        "var1": [1, 5, 9, 13],
        "var2": [3, 7, 11, 15],
        "date": ["2022-01-01", "2022-01-02", "2023-01-01", "2023-01-02"],
        "sample": ["sample1", "sample1", "sample2", "sample2"]
    }
)

df.dtypes
```

To handle temporal data, it is necessary to convert the strings into `datetime` objects. Pandas does this via the `to_datetime()` function.

```{python}
df['date'] = pd.to_datetime(df['date'])

df.dtypes
```

Once converted, dates can be formatted, compared, and used in calculations. In particular, Pandas now understands the "order" of the dates present in the data, allowing for filtering over given periods.

```python
df[(df['date'] >= "2022-01-01") & (df['date'] < "2022-01-03")]
```

We might also want to perform less precise filtering, involving the year or month. Pandas allows us to easily extract specific components of the date, such as the year, month, day, hour, etc.

```python
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day

df[df['year'] == 2023]
```

Finally, calculations involving dates become possible. We can add or subtract time periods from dates and compare them with each other. The functions used come from `Pandas` but are very similar in operation to those of the [time](https://docs.python.org/fr/3/library/time.html) module in Python.

For example, we can add time intervals or calculate differences from a reference date.

```python
df['date_plus_one'] = df['date'] + pd.Timedelta(days=1)
df['date_diff'] = df['date'] - pd.to_datetime('2022-01-01')

df
```

### Joining tables

In data analysis, it is common to want to combine different data sources. This combination can be done vertically (one DataFrame on top of another), for example, when combining two years of the same survey for joint analysis. The combination can also be done horizontally (side by side) based on one or more join keys, often to enrich one data source with information from another source covering the same statistical units.

#### Concatenating tables

The vertical concatenation of tables is done using the `concat()` function in Pandas.

```{python}
df1 = pd.DataFrame(
    data = {
        "var1": [1, 5],
        "var2": [3, 7],
        "date": ["2022-01-01", "2022-01-02"],
        "sample": ["sample1", "sample1"]
    }
)

df2 = pd.DataFrame(
    data = {
        "var1": [9, 13],
        "date": ["2023-01-01", "2023-01-02"],
        "var2": [11, 15],
        "sample": ["sample2", "sample2"]
    }
)

df_concat = pd.concat([df1, df2])

df_concat
```

Note that the order of variables in the two DataFrames is not important. Pandas does not "dumbly" juxtapose the two DataFrames; it matches the schemas to align the variables by name. If two variables have the same name but not the same type - for example, if a numeric variable has been interpreted as strings - Pandas will resolve the issue by taking the common denominator, usually converting to strings (type `object`).

However, the previous concatenation reveals an issue of repetition at the index level. This is logical: we did not specify an index for our initial two DataFrames, which therefore have the same position index ([0, 1]). In this case (where the index is not important), we can pass the `ignore_index=True` parameter to rebuild the final index from scratch.

```{python}
df_concat = pd.concat([df1, df2], ignore_index=True)

df_concat
```

::: {.callout-warning title="Iterative construction of a DataFrame"}
One might have the idea of using `pd.concat()` to iteratively construct a DataFrame by adding a new row to the existing DataFrame in each iteration of a loop. However, this is not a good idea: as we have seen, a DataFrame is represented in memory as a juxtaposition of Series. Thus, adding a column to a DataFrame is not costly, but adding a row involves modifying each element constituting the DataFrame. To construct a DataFrame, it is therefore advisable to store the rows in a list of lists (one per column) or a dictionary, then call `pd.DataFrame()` to build the DataFrame, as we did at the beginning of this tutorial.
:::

#### Merging tables

Merging tables is an operation that allows us to associate rows from two different DataFrames based on one or more common keys, similar to joins in SQL databases. Different types of joins are possible depending on the data we want to keep, the main ones being represented in the following diagram.

![](img/joins.png)

Source: [link](https://medium.com/swlh/merging-dataframes-with-pandas-pd-merge-7764c7e2d46d)

In Pandas, joins are done with the [merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function. To perform a join, we must specify (at a minimum) two pieces of information:

- the type of join: by default, Pandas performs an `inner` join. The `how` parameter allows specifying other types of joins;

- the join key. By default, Pandas tries to join the two DataFrames based on their indexes. In practice, we often specify a variable present in the DataFrames as the join key (the `on` parameter if the variable has the same name in both DataFrames, or `left_on` and `right_on` otherwise).

Let's analyze the difference between the different types of joins through examples.

```{python}
df_a = pd.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K3', 'K4'],
    'A': ['A0', 'A1', 'A2', 'A3', 'A4'],
    'B': ['B0', 'B1', 'B2', 'B3', 'A4']
})

df_b = pd.DataFrame({
    'key': ['K0', 'K1', 'K2', 'K5', 'K6'],
    'C': ['C0', 'C1', 'C2', 'C5', 'C6'],
    'D': ['D

0', 'D1', 'D2', 'D5', 'D6']
})

display(df_a)
display(df_b)
```

The `inner` join keeps the observations whose key is present in both DataFrames.

```{python}
df_merged_inner = pd.merge(df_a, df_b, on='key')
df_merged_inner
```

::: {.callout-warning title="Inner joins"}
The `inner` join is the most intuitive: it generally does not create missing values and therefore allows working directly on the merged table. But beware: if many keys are not present in both DataFrames, an `inner` join can result in significant data loss, leading to biased final results. In this case, it is better to choose a left or right join, depending on the source we want to enrich and for which it is most important to minimize data loss.
:::

A `left` join keeps all observations in the left DataFrame (the first DataFrame specified in `pd.merge()`). As a result, if keys are present in the left DataFrame but not in the right one, the final DataFrame contains missing values at those observations (for the right DataFrame's variables).

```{python}
df_merged_left = pd.merge(df_a, df_b, how="left", on='key')
df_merged_left
```

The `outer` join contains all observations and variables in both DataFrames. Thus, the retained information is maximal, but on the other hand, missing values can be quite numerous. It will therefore be necessary to handle missing values well before proceeding with analyses.

```{python}
df_merged_outer = pd.merge(df_a, df_b, how="outer", on='key')
df_merged_outer
```

## Exercises

### Comprehension questions

1. What is a DataFrame in the context of Pandas, and what type of data structure can it be compared to in Python?
2. What is the fundamental difference between a NumPy array and a Pandas Series?
3. What is the relationship between Series and DataFrame in Pandas?
4. How are data structured in a Pandas DataFrame?
5. What is the role of the index in a Pandas DataFrame, and how can it be used when manipulating data?
6. What methods can you use to explore an unknown DataFrame and learn more about its content and structure?
7. In Pandas, what is the difference between assigning the result of an operation to a new variable and using a method with the `inplace=True` argument?
8. How does the principle of vectorization apply in Pandas, and why is it advantageous for manipulating data?
9. How does Pandas represent missing values, and what impact does this have on calculations and data transformations?
10. What is the difference between concatenating two DataFrames and joining them via a merge, and when would you use one over the other?

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

1. A DataFrame in Pandas is a two-dimensional data structure, comparable to a table or an Excel spreadsheet. In the Python context, it can be compared to a dictionary of NumPy arrays, where the keys are column names, and the values are the columns themselves.

2. The main difference between a NumPy array and a Pandas Series is that the Series can contain labeled data, meaning it has an associated index that allows access and manipulation by label.

3. A DataFrame is essentially a collection of Series. Each column of a DataFrame is a Series, and all these Series share the same index, which corresponds to the row labels of the DataFrame.

4. Data in a Pandas DataFrame are structured in columns and rows. Each column can contain a different type of data (numeric, string, boolean, etc.), and each row represents an observation.

5. The index in a Pandas DataFrame serves to uniquely identify each row in the DataFrame. It allows quick access to rows, performing joins, sorting data, and facilitating grouping operations.

6. To explore an unknown DataFrame, you can use `df.head()` to see the first rows, `df.tail()` for the last rows, `df.info()` to get a summary of data types and missing values, and `df.describe()` for descriptive statistics.

7. Assigning the result of an operation to a new variable creates a copy of the DataFrame with the applied modifications. Using a method with `inplace=True` modifies the original DataFrame without creating a copy, which can be more memory-efficient.

8. Pandas represents missing values with the `nan` (Not a Number) object from `NumPy` for numeric data and with `None` or `pd.NaT` for date/time data. These missing values are generally ignored in statistical calculations, which can affect the results if they are not handled properly.

9. Concatenating consists of stacking DataFrames vertically or aligning them horizontally, primarily used when the DataFrames have the same schema or when you want to stack the data. Merging, inspired by SQL JOIN operations, combines DataFrames based on common key values and is used to enrich one dataset with information from another.

</details>

:::

### Multiple ways to create a DataFrame

In the following cell, we have retrieved cash register data on sales from different stores. The data is presented in two different ways: one as observations (each list contains data from a row), and the other as variables (each list contains data from a column).

```{python}
data_list1 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

data_list2 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]
```

The goal is to build in both cases the same DataFrame containing each of the 6 observations and 4 variables, with the same names in both DataFrames. Each case will correspond to a more suitable input data structure, dictionary, or list of lists... make the right choice! We will verify that the two DataFrames are identical using the [equals()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.equals.html) method.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
data_list1 = [
    ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    [3, 2, 7, 5, 10, 1],
    [1.50, 2.30, 0.99, 5.00, 1.20, 3.10]
]

data_list2 = [
    ['Carrefour', '01.1.1', 3, 1.50],
    ['Casino', '02.1.1', 2, 2.30],
    ['Lidl', '01.1.1', 7, 0.99],
    ['Carrefour', '03.1.1', 5, 5.00],
    ['Casino', '01.1.1', 10, 1.20],
    ['Lidl', '02.1.1', 1, 3.10]
]

# If the data is in column form: from a dictionary
data_dict = {
    'store': data_list1[0],
    'product': data_list1[1],
    'quantity': data_list1[2],
    'price': data_list1[3]
}

df_from_dict = pd.DataFrame(data_dict)

# If the data is in row form: from a list of lists
columns = ['store', 'product', 'quantity', 'price']
df_from_list = pd.DataFrame(data_list2, columns=columns)

# Verification
df_from_dict.equals(df_from_list)
```

</details>

:::

### Data selection in a DataFrame

A Pandas DataFrame is created with cash register data (same data as the previous exercise).

```{python}
data = {
    'store': ['Carrefour', 'Casino', 'Lidl', 'Carrefour', 'Casino', 'Lidl'],
    'product': ['01.1.1', '02.1.1', '01.1.1', '03.1.1', '01.1.1', '02.1.1'],
    'quantity': [3, 2, 7, 5, 10, 1],
    'price': [1.50, 2.30, 0.99, 5.00, 1.20, 3.10],
    'date_time': pd.to_datetime(["2022-01-01 14:05", "2022

-01-02 09:30", 
                                 "2022-01-03 17:45", "2022-01-04 08:20", 
                                 "2022-01-05 19:00", "2022-01-06 16:30"])
}

df = pd.DataFrame(data)
```

Use the `loc` and `iloc` methods to select specific data:

- Select the data from the first row.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.iloc[0])
```

</details>

:::

- Select all data from the "price" column.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[:, 'price'])
```

</details>

:::

- Select the rows corresponding to the store "Carrefour" only.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['store'] == 'Carrefour'])
```

</details>

:::

- Select the quantities purchased for products classified "01.1.1" (Bread).

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['product'] == '01.1.1', 'quantity'])
```

</details>

:::

- Select the data from the "store" and "price" columns for all rows.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[:, ['store', 'price']])
```

</details>

:::

- Select the rows where the purchased quantity is greater than 5.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['quantity'] > 5])
```

</details>

:::

- Filter to select all transactions that occurred after 3 PM.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['date_time'].dt.hour > 15])
```

</details>

:::

- Select the transactions that took place on "2022-01-03".

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df.loc[df['date_time'].dt.date == pd.to_datetime('2022-01-03').date()])
```

</details>

:::

### Exploring the first names file

The first names file contains data on the first names given to children born in France between 1900 and 2021. This data is available at the national, department, and regional levels at the following address: [https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262). The goal of this tutorial is to propose an analysis of this file, from data cleaning to first name statistics.

#### Part 1: Import and data exploration

- Import the data into a DataFrame using this [URL](https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip).
- View a sample of the data. Do you notice any anomalies?
- Display the main information about the DataFrame. Identify any variables with incorrect types or any missing values.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
url = "https://www.insee.fr/fr/statistiques/fichier/2540004/nat2021_csv.zip"
df_first_names = pd.read_csv(url, sep=";")

df_first_names.head(10)
df_first_names.sample(n=50)

df_first_names.info()
```

</details>

:::

#### Part 2: Data cleaning

- The output of the `info()` method suggests missing values in the first names column. Display these rows. Verify that these missing values are correctly specified.
- The output of the `head()` method shows a recurring "_PRENOMS_RARES" modality in the first names column. What proportion of the individuals in the database does this represent? Convert these values to `np.nan`.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
print(df_first_names[df_first_names["preusuel"].isna()])
prop_rares = df_first_names.groupby("preusuel")["nombre"].sum()["_PRENOMS_RARES"] / df_first_names["nombre"].sum()
print(prop_rares)  # ~ 2% of the database
df_first_names = df_first_names.replace('_PRENOMS_RARES', np.nan)
```

</details>

:::

- We notice that the first names of people whose year of birth is unknown are grouped under the "XXXX" modality. What proportion of the individuals in the database does this represent? Convert these values to `np.nan`.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
prop_xxxx = df_first_names.groupby("annais")["nombre"].sum()["XXXX"] / df_first_names["nombre"].sum()
print(prop_xxxx)  # ~ 1% of the database
df_first_names = df_first_names.replace('XXXX', np.nan)
```

</details>

:::

- Remove the rows containing missing values from the sample.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names = df_first_names.dropna()
```

</details>

:::

- Convert the `annais` column to numeric type and the `sexe` column to categorical type.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names['annais'] = pd.to_numeric(df_first_names['annais'])
df_first_names['sexe'] = df_first_names['sexe'].astype('category')
```

</details>

:::

- Verify with the `info()` method that the cleaning has been correctly applied.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names.info()
```

</details>

:::

#### Part 3: Descriptive statistics on births

- The [documentation](https://www.insee.fr/fr/statistiques/2540004?sommaire=4767262#documentation) of the file informs us that the data can be considered quasi-exhaustive from 1946 onwards. For this part only, filter the data to keep only data from 1946 onwards.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_first_names_post_1946 = df_first_names[df_first_names["annais"] >= 1946]
```

</details>

:::

- Calculate the total number of births by sex.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
births_per_sex = df_first_names_post_1946.groupby('sexe')['nombre'].sum()
print(births_per_sex)
```

</details>

:::

- Identify the five years with the highest number of births.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
top5_years = df_first_names_post_1946.groupby('annais')['nombre'].sum().nlargest(5)
print(top5_years)
```

</details>

:::

#### Part 4: First name analysis

- Identify the total number of unique first names in the DataFrame.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
total_unique_names = df_first_names['preusuel'].nunique()
print(total_unique_names)
```

</details>

:::

- Count the number of people with a single-letter first name.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
single_letter_names = df_first_names[df_first_names['preusuel'].str.len() == 1]['nombre'].sum()
print(single_letter_names)
```

</details>

:::

- Create a "popularity function" that, for a given first name, displays the year it was most given and the number of times it was given that year.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
def popularity_by_year(df, first_name):
    # Filter the DataFrame to keep only the rows corresponding to the given first name
    df_first_name = df[df['preusuel'] == first_name]

    # Group by year, sum the births, and identify the year with the maximum births
    df_agg = df_first_name.groupby('annais')['nombre'].sum()
    max_year = df_agg.idxmax()
    max_n = df_agg[max_year]

    print(f"The first name '{first_name}' was most given in {max_year}, with {max_n} births.")

# Test the function with an example
popularity_by_year(df_first_names, 'ALFRED')
```

</details>

:::

- Create a function that, for a given sex, returns a DataFrame containing the most given first name for each decade.

```{python}
# Test

 your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
def popularity_by_decade(df, sex):
    # Filter by sex
    df_sub = df[df["sexe"] == sex]

    # Calculate the decade variable
    df_sub["decade"] = (df_sub["annais"] // 10) * 10

    # Calculate the sum of births for each first name and each decade
    df_counts_decade = df_sub.groupby(["preusuel", "decade"])["nombre"].sum().reset_index()

    # Find the index of the most frequent first name for each decade
    idx = df_counts_decade.groupby("decade")["nombre"].idxmax()

    # Use the index to obtain the corresponding rows from the df_counts_decade DataFrame
    df_popularity_decade = df_counts_decade.loc[idx].set_index("decade")

    return df_popularity_decade

# Test the function with an example
popularity_by_decade(df_first_names, sex=2)
```

</details>

:::

### Calculation of a carbon footprint per inhabitant at the municipal level

The goal of this exercise is to calculate a carbon footprint per inhabitant at the municipal level. To do this, we will need to combine two data sources:

- Legal populations at the municipal level from the population census ([source](https://www.insee.fr/fr/statistiques/6683037))

- Greenhouse gas emissions estimated at the municipal level by ADEME ([source](https://www.data.gouv.fr/fr/datasets/inventaire-de-gaz-a-effet-de-serre-territorialise/#_))

This exercise constitutes a simplified version of a [complete practical exercise on Pandas](https://pythonds.linogaliana.fr/content/manipulation/02b_pandas_TP.html#importer-les-donn%C3%A9es) proposed by Lino Galiana in his [course at ENSAE](https://pythonds.linogaliana.fr/).

#### Part 1: Exploring the legal municipal populations data

- Import the CSV file `communes.csv`.
- Use the `.sample()`, `.info()`, and `.describe()` methods to get an overview of the data.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_pop_communes = pd.read_csv("data/communes.csv", sep=";")

df_pop_communes.sample(10)
df_pop_communes.info()
df_pop_communes.describe()
```

</details>

:::

- Identify and remove rows corresponding to municipalities without population.
- Remove the "PMUN" and "PCAP" columns, which are irrelevant for the analysis.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
n_communes_0_pop = df_pop_communes[df_pop_communes["PTOT"] == 0].shape[0]
print(n_communes_0_pop)
df_pop_communes = df_pop_communes[df_pop_communes["PTOT"] > 0]

df_pop_communes = df_pop_communes.drop(columns=["PMUN", "PCAP"])
```

</details>

:::

Do the municipalities with the longest names also have the smallest populations? To find out:
- Create a new variable that contains the number of characters of each municipality using the [str.len()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.len.html) method.
- Calculate the correlation between this variable and the total population using the [corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) method.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_pop_communes_stats = df_pop_communes.copy()
df_pop_communes_stats['length'] = df_pop_communes_stats['COM'].str.len()
df_pop_communes_stats['length'].corr(df_pop_communes_stats['PTOT'])
```

</details>

:::

#### Part 2: Exploring the municipal emissions data

- Import the emissions data from this [URL](https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv).
- Use the `.sample()`, `.info()`, and `.describe()` methods to get an overview of the data.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
url_ademe = "https://data.ademe.fr/data-fair/api/v1/datasets/igt-pouvoir-de-rechauffement-global/data-files/IGT%20-%20Pouvoir%20de%20r%C3%A9chauffement%20global.csv"
df_emissions = pd.read_csv(url_ademe)

df_emissions.sample(10)
df_emissions.info()
df_emissions.describe()
```

</details>

:::

- Are there rows with missing values for all emission columns? Check using the [isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) and [all()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.all.html) methods.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions_num = df_emissions.select_dtypes(['number'])
only_nan = df_emissions_num[df_emissions_num.isnull().all(axis=1)]
only_nan.shape[0]
```

</details>

:::

- Create a new column that gives the total emissions per municipality.
- Display the 10 most emitting municipalities. What do you observe in the results?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions['total_emissions'] = df_emissions.sum(axis=1, numeric_only=True)

df_emissions.sort_values(by="total_emissions", ascending=False).head(10)
```

</details>

:::

- It seems that the major emission sectors are "Industry excluding energy" and "Other international transport." To verify if this conjecture holds, calculate the correlation between total emissions and the sectoral emission items using the [corrwith()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corrwith.html) method.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions.corrwith(df_emissions["total_emissions"], numeric_only=True)
```

</details>

:::

- Extract the department number from the municipality code into a new variable.
- Calculate the total emissions by department.
- Display the top 10 emitting departments. Are the results logical compared to the analysis at the municipal level?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions["dep"] = df_emissions["INSEE commune"].str[:2]
df_emissions.groupby("dep").agg({"total_emissions": "sum"}).sort_values(by="total_emissions", ascending=False).head(10)
```

</details>

:::

#### Part 3: Preliminary checks for merging data sources

To perform a merge, it is always preferable to have a join key, i.e., a column common to both sources that uniquely identifies the statistical units. The purpose of this part is to find the relevant join key.

- Check if the variable containing the municipality names contains duplicates.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
duplicates = df_pop_communes.groupby('COM').count()['DEPCOM']
duplicates = duplicates[duplicates > 1]
duplicates = duplicates.reset_index()
duplicates
```

</details>

:::

- Filter in the initial DataFrame the municipalities with duplicated names and sort it by municipality code. Do the duplicates seem problematic?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_pop_communes_duplicates = df_pop_communes[df_pop_communes["COM"].isin(duplicates["COM"])]
df_pop_communes_duplicates.sort_values('COM')
```

</details>

:::

- Verify that the municipality codes uniquely identify the associated municipality.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
(df_pop_communes_duplicates.groupby("DEPCOM")["COM"].nunique() != 1).sum()
```

</details>

:::

- Display the municipalities present in the population data but not in the emissions data, and vice versa. What do you conclude?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
# Observations in the population data but not in the emissions data
df_pop_communes[~df_pop_communes["DEPCOM"].isin(df_emissions["INSEE commune"])]

# Observations in the emissions data but not in the population data
df_emissions[~df_emissions["INSEE commune"].isin(df_pop_communes["DEPCOM"])]
```

</details>

:::

#### Part 4: Calculating a carbon footprint per inhabitant for each municipality

- Merge the two DataFrames using the municipality code as the join key. Note: the variables are not named the same on both sides!

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions_pop = pd.merge(df_pop_communes

, df_emissions, how="inner", left_on="DEPCOM", right_on="INSEE commune")
df_emissions_pop
```

</details>

:::

- Calculate a carbon footprint for each municipality, corresponding to the total emissions of the municipality divided by its total population.
- Display the top 10 municipalities with the highest carbon footprints.
- Are the results the same as those with total emissions? What do you conclude?

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_emissions_pop["carbon_footprint"] = df_emissions_pop["total_emissions"] / df_emissions_pop["PTOT"]
df_emissions_pop.sort_values("carbon_footprint", ascending=False).head(10)
```

</details>

:::

### Analysis of the evolution of a production index

You have two CSV data sets available in the `data/` folder:
- `serie_glaces_valeurs.csv` contains the monthly values of the production price index of the French ice cream and sorbet industry.
- `serie_glaces_metadonnees.csv` contains the associated metadata, including the codes indicating the data status.

The goal is to use `Pandas` to calculate:
- the evolution of the index between each period (month)
- the evolution of the index on a year-over-year basis (between a given month and the same month the following year).

#### Part 1: Importing data

- Import the two CSV files into DataFrames. Note: in both cases, there are extraneous rows before the data that need to be skipped using the `skiprows` parameter of the [read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function.
- Give simple and relevant names to the various variables.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_values = pd.read_csv('data/serie_glaces_valeurs.csv', delimiter=';',
                        skiprows=4, names=["period", "index", "code"])
df_metadata = pd.read_csv('data/serie_glaces_metadonnees.csv', delimiter=';',
                          skiprows=5, names=["code", "meaning"])
```

</details>

:::

#### Part 2: Filtering relevant data

- Merge the two DataFrames to retrieve the meanings of the codes present in the data.
- Filter the data to keep only the "Normal Value" data.
- Remove the columns related to the codes, which we no longer need for the rest.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_merged = pd.merge(df_values, df_metadata, how='left', on='code')

df_clean = df_merged[df_merged['code'] == "A"]
df_clean = df_clean[["period", "index"]]
```

</details>

:::

#### Part 3: Data preprocessing

Verify if the types of variables are relevant according to their nature. If not, convert them with the appropriate functions.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_clean.info()
df_clean['period'] = pd.to_datetime(df_clean['period'])
df_clean['index'] = pd.to_numeric(df_clean['index'])
df_clean.info()
```

</details>

:::

#### Part 4: Calculating periodic evolution

- Use the [shift()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) method to create a new column containing the previous month's index.
- Calculate the difference between the current index and the shifted index to obtain the (percentage) evolution from one month to the next.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_clean['previous_index'] = df_clean['index'].shift(1)
df_clean['evolution'] = ((df_clean['index'] - df_clean['previous_index']) / df_clean['previous_index']) * 100

# Alternative method
df_clean['alternative_evolution'] = df_clean['index'].pct_change(periods=1) * 100
```

</details>

:::

#### Part 5: Calculating year-over-year evolution

As you saw in the previous exercise's solution, the [pct_change()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html) method allows you to calculate an evolution between two periods. Use this method to calculate a year-over-year evolution for each month.

```{python}
# Test your answer in this cell
```

::: {.cell .markdown}

<details>
<summary>Show solution</summary>

```{python}
df_clean["year_over_year_evolution"] = df_clean['index'].pct_change(periods=12) * 100
df_clean.head(20)
```

</details>

:::


:::